\section{Discussion}
\label{section: discussion}
In this thesis, the performance of SSL methods on OoDD for chest-x-ray images was examined and compared to supervised methods.
The performance was evaluated on three different settings.
It was not possible to outperform the supervised baseline methods in \citep{Berger2021} with the unsupervised feature similarity OOD score.
However, comparable results to the supervised baseline methods were achieved when the pretrained ViT encoders were fine-tuned.
\par
In the first setting (ID: \textit{No Finding}, OOD: \textit{Cardiomegaly}, \textit{Fracture}, \textit{Lung Opacity}, \textit{Pleural Effusion}, \textit{Pneumothorax}, \textit{Support Devices}) outlier exposure and custom augmentations were applied.
Unsupervised outlier exposure worked even with quite small label fractions, which is useful in the real world as although access to OOD samples is needed for better results, no fine-grained OOD class labels are needed (see figure \ref{fig:ood-fraction-avg}).
The performance improvements against the unsupervised baseline (see figure \ref{fig:setting1-chexnorm-v-imgnorm-dino-last-epoch}), were only achieved for a small subset of pathologies and the fracture class still showed no large improvements (see figure \ref{fig:ood-fraction-fracture}).
No performance increases were observed for applying custom augmentations specifically tailored for the fracture class (see table \ref{table:custom-augs-fracture-results}).
The augmentations can be interpreted as heuristic and no performance gain could be recognized.
A potential reason for this could be, that the fracture class may look quite similar to the control class and the region of interest may be too small. 
Image enhancement techniques could be applied to intensify the small local texture region of interests.
Koonsanit et. al. propose N-CLAHE (Normalized Contrast Limited Adaptive Histogram Equalization) as an image enhancement technique \citep{Koonsanit2017}.
It is based on the CLAHE algorithm, which is a contrast enhancement technique that uses several histograms of distinct image sections to improve the contrast of images \citep{Zuiderveld1994}.
N-CLAHE is a modified version of CLAHE, which normalizes pixel intensity values before applying the CLAHE algorithm.
The method was successfully applied to Chest-X-ray images as a preprocessing step for COVID-19 detection and improved textural details of the images \citep{Horry2020}.
As the developed custom augmentations did not work, this could pose an interesting research direction.
\par 
In the second setting (ID: \textit{Cardiomegaly}, \textit{Pneumothorax} OOD: \textit{Fracture}), DINO achieved slightly better results than the supervised baseline when fine-tuned (see table \ref{table:finetuning-supervised-new}).
The pretrained weights extracted by the DINO learning objective proved to be useful.
Still, the state-of-the-art performance (SOTA) of ODIN could not be reached.
Interestingly, the authors in \citep{Berger2021} conduct an ablation study and show that perturbation and not the temperatures scaling leads to the high performance of ODIN on chest-X-rays.
The ineffectiveness of temperature scaling was also observed in this thesis (see fig \ref{fig:auroc-vs-temp}).
It could be interesting to incorporate perturbations into the proposed unsupervised OoDD framework.
% Further, it is unclear if the results of ODIN are reproducible to other datasets.
% Berger et. al. did not use OOD samples from the CheXpert dataset
% The results of ODIN should be reproduced on other pathologies and it should be evaluated if the perturbation is also effective for other pathologies and datasets.

% Applying generalized ODIN (GODIN) \citep{Hsu2020} could also be a promising research direction.
% The proposed method is similar to ODIN, e.g. input preprocessing is also applied to the images \citep{Hsu2020}, but it does not require access to OOD samples.
% \sout{However, ODIN assumes access to the OOD class labels that poses a limitation in the real world. 
% A potential way to overcome this issue would be to apply generalized ODIN (GODIN) \citep{Hsu2020}.
% The proposed method is similar to ODIN, e.g. input preprocessing is also applied to the images \citep{Hsu2020}.
% In contrast to ODIN it does not require access to OOD samples and a comparison between the performance of GODIN and ODIN on chest-x-ray OoDD could be insightful.}
\par
No satisfactory results were achieved for the third setting (ID: \textit{Lung Opacity}, \textit{Pleural Effusion}, OOD: \textit{Fracture}, \textit{Pneumonia}).
One of the possible reasons could be the similarity between the super ID class lung opacity and OOD class pneumonia that results from the multi-label structure. 
Therefore, the trained models might confuse the two classes which degrades the OoDD performance. 
Another classifier based approach would be to learn the sub labels of the super ID class to better separate it from the OOD class, still requiring label knowledge.
One could also apply outlier exposure to setting one, which proved to be successful in setting one.
%\sout{Similar to setting two, applying GODIN could also be a promising research direction.}
\par
The experiments were conducted on a limited scope of the larger problem.
Especially the second and third setting contain at most two ID classes and two OOD classes.
The findings should be evaluated on different Chest X-ray datasets e.g. the NIH Chest-X-ray dataset \citep{Wang2017} and across the ID-OOD splits developed in \citep{Cao2020} to ensure validity and generalizability.
For example, one ID-OOD splits for novel disease detection developed in \citep{Cao2020} contains ten ID classes and four OOD classes.
Reproducing the results on dataset splits of larger scale is necessary for a more comprehensive evaluation.
\\
To ensure consistency with \citep{Berger2021}, no label overlap between ID classes was assumed. 
In reality, this is not a plausible assumption as the occurrence of pathologies is not mutually exclusive.
Results of this thesis should therefore also be evaluated on dataset splits with label overlap and OoDD methods should be modified to handle the multi-label case.
For unsupervised OoDD with feature similarity as anomaly score this would not make a difference and the same approach could be applied.
However, for supervised methods, the OoDD methods would need to be modified to handle the multi-label case.
Hendrycks et al. recently proposed a method to handle the multi-label case for supervised OoDD and use the logistic sigmoid function for multi label classification and instead of MSP, they apply the maximum unnormalized logit as an anomaly score \citep{Hendrycks2022}.
% In addition, the resolution of the processed images is low. 
% Small details of local textures might be missed and affect the pathology detection. 
% Existing research methods could be applied to integrate images of higher resolution [SOURCES suchen].
\par
CheXpert labels were used as ground truth labels for the experiments.
However, the labels are noisy as they are extracted by NLP methods (see \ref{section: dataset}) and are also assigned uncertainty labels.
In addition, the label quality might be inconsistent across the different pathologies.
Abdalla et. al. examine the label quality of the CheXpert dataset in a recent study \citep{Abdalla2023}.
They evaluate the consistency of the official gold standard CheXpert labels and conclude that the label noise deviates by pathology \citep{Abdalla2023}.
The gold standard labels are based on the majority vote of five radiologists randomly chosen from a pool of eight radiologists \citep{Irvin2019}.
The authors suppose, that the quality of the gold label set depends on the choice of the radiologists and randomly sample from the pool of eight radiologists to evaluate the agreement of all possible pairings \citep{Abdalla2023}. 
While pneumothorax and lung opacity show high label consistency, the median agreement between the radiologist's labels for the fracture class is merely 50\% \citep{Abdalla2023}.
Actual reported performances in this thesis may therefore be lower than the reported performances.
Nguyen et. al. released a new dataset called VinDr-CXR that contains 18,000 chest x-ray images that were manually annotated by a group of 17 radiologists \citep{Nguyen2022}.
One way to deal with the label inconsistency could be to pretrain the DINO models on the CheXpert dataset and then fine-tune on the VinDr-CXR dataset to evaluate the consistency across reported performances.
\par
Apart from images, radiology reports are a further source of information that could be leveraged. 
Experts opinions of radiologists are often based on the combination of both. 
Datasets such as MIMIC-CXR \citep{Johnson2019} provide both images and reports. 
Multimodal SSL techniques such as CLIP \citep{Radford2021} can combine text and image data, and it was recently that they are powerful OOD detectors across a large cohort of datasets \citep{Michels2023}.
Tiu et. al. already applied CLIP to chest-x-ray pathology classification \citep{Tiu2022}.
They trained a CLIP model on the MIMIC-CXR dataset that learns to match the chest X-ray images with their corresponding radiology reports \citep{Tiu2022}.
Importantly they do not use the labels of the images for training and rather leverage the available text reports \citep{Tiu2022}.
This could improve on the problem of noisy labels and label inconsistency and could be a promising research direction for future work.
\par
Further, as DINO has already produced promising results, one could also apply the all-purpose feature extractor DINOv2 \citep{Oquab2023} to OoDD of Chest-X-ray images. 
Research should further put weight on exploring how the global crop scale of the DINO learning objective relates to the performance.
A larger global crop scale could be beneficial for chest-X-rays as otherwise, small pathology regions might be missed.
Finally, ablation studies should be carried out, that explore the impact of the ViT patch size on the OoDD performance on x-ray images.
Especially for small textural chest-x-ray incongruities, decreased patch sizes might be advisable.
\par
In conclusion, SSL can be adopted to novel disease OoDD for chest-x-ray images with few adjustments needed.
The unsupervised performance does not match the supervised baseline methods, but fine-tuned results are comparable.
%Interestingly, for outlier exposure, no access to fine-grained OOD class labels is needed, which is different to standard computer vision benchmarks [SOURCES] and useful for real world adoption.
Overall, pretraining DINO on chest-x-ray images and using the pretrained weights for OoDD is a promising research direction for future work.