\section{Discussion}
\label{section: discussion}
In this thesis, the performance of SSL methods for OOD detection in chest X-rays was investigated and compared to supervised methods.
It was not possible to outperform the supervised baseline methods in \citep{Berger2021} with the unsupervised feature similarity OOD score.
However, comparable results to the supervised baseline methods were achieved when the pretrained ViT encoders were fine-tuned.
Outlier exposure was also applied and lead to performance improvements.
\par
A total of three settings were examined.
The first setting contains one ID class and six OOD classes (ID: \textit{No Finding}; OOD: \textit{Cardiomegaly}, \textit{Fracture}, \textit{Lung Opacity}, \textit{Pleural Effusion}, \textit{Pneumothorax}, \textit{Support Devices}), the second setting contains two ID classes and one OOD class (ID: \textit{Cardiomegaly}, \textit{Pneumothorax}; OOD: \textit{Fracture}), and the third setting two ID classes and two OOD classes (ID: \textit{Lung Opacity}, \textit{Pleural Effusion}; OOD: \textit{Fracture}, \textit{Pneumonia}).
\par
\textbf{Effect of ImageNet normalization.} ImageNet and CheXpert normalization were compared as a preprocessing step.
It was shown, that ImageNet normalization leads to slight performance improvements for pretraining with DINO (see Figure \ref{fig:setting1-chexnorm-v-imgnorm-dino-barplot}).
ImageNet is also commonly used in other studies for chest X-rays \citep{Azizi2021,Pham2020}.
Due to the low standard deviation of the CheXpert dataset, CheXpert normalization normalizes images to high input values, which can lead to convergence problems \citep{Lecun2002, He2015, Santurkar2019}.
ImageNet normalization is therefore favorable to CheXpert normalization.
\par
\textbf{Adding rotations to the augmentations.}
The effect of adding rotations to the augmentations was examined, and it was shown that unsupervised OOD detection performance was slightly worse when rotations were added to the augmentations.
The 1-NN mean accuracy however increased for setting 2 and for setting 3 no clear trend could be established (see Figure \ref{fig:dino-rotate-augs-v-default-augs}).
In fact, no clear trend could be observed between the 1-NN mean accuracy and the unsupervised OOD detection performance independent of the applied augmentations.
This is different to natural images: For example Rafiee et al. \citep{Rafiee2022} establish a correlation between the 10-NN mean ID accuracy and unsupervised OOD detection performance for near- and far OOD on natural images.
The effectiveness of adding rotations to the augmentations for OOD detection of chest X-ray images should be further examined as the supervised OOD performance was significantly higher for setting 2 when rotations were added to the pretraining augmentations and fine-tuning was applied.
In a recent study, Elgendi et al. \citep{Elgendi2021} investigate the effectiveness of geometric augmentations (e.g. rotations) to detect COVID-19 in chest X-rays.
They conclude that geometric augmentations are not effective for COVID-19 detection and hypothesize \citep{Elgendi2021} that \textit{Pneumonias} tend to be of unspecific size and shape and therefore geometric augmentations might not be effective.
As setting 3 contains the unspecific \textit{Lung Opacity} as ID class, this could be a reason for the ineffectiveness of rotations.
Further, one can hypothesize that \textit{Cardiomegaly} and \textit{Pneumothorax} often share a more "typical pattern" and therefore rotations are more effective for setting 2 and lead to higher ID accuracy.
In the following, each setting is discussed in detail.
\par
% Apart from the performance, the low standard deviation of the CheXpert dataset statistics ($\sigma=0.0349$), leads to a normalized input range of approximately [-17,14].
% Therefore, to mitigate the risk of convergence problems for high input values \citep{Lecun2002, He2015, Santurkar2019} and because of higher performance, the ImageNet normalization is used for all following experiments.
\textbf{Outlier exposure and custom augmentations.}
In the first setting, outlier exposure and custom augmentations were applied.
Unsupervised outlier exposure worked even with quite small label fractions, which is helpful in the real world as although access to OOD samples is needed for better results, no fine-grained OOD class labels are required (see Figure \ref{fig:ood-fraction-avg}).
This is consistent with the findings of \citep{Fort2021} that showed that outlier exposure with pretrained ViTs is effective for OOD detection on natural images and genomic data and that access to fine-grained OOD labels only leads to minor improvements against ignoring the outlier labels.
The performance improvements against the unsupervised baseline (see Table \ref{fig:setting1-chexnorm-v-imgnorm-dino-last-epoch}), were only achieved for a small subset of OOD pathologies and the \textit{Fracture} class still showed no significant improvements (see Figure \ref{fig:ood-fraction-fracture}).
No performance increases were observed for applying custom augmentations tailored explicitly for the \textit{Fracture} class (see Table \ref{table:custom-augs-fracture-results}).
The augmentations can be interpreted as heuristic and no performance gain could be recognized.
A potential reason for this is, that the \textit{Fracture} class may look quite similar to the control class and the region of interest may be too small. 
Image enhancement techniques could be applied to intensify the small local texture region of interest.
Koonsanit et al. propose N-CLAHE (Normalized Contrast Limited Adaptive Histogram Equalization) as an image enhancement technique \citep{Koonsanit2017}.
It is based on the CLAHE algorithm, which is a contrast enhancement technique that uses several histograms of distinct image sections to improve the contrast of images \citep{Zuiderveld1994}.
N-CLAHE is a modified version of CLAHE, which normalizes pixel intensity values before applying the CLAHE algorithm.
The method was successfully applied to chest X-ray images as a preprocessing step for COVID-19 detection and improved textural details of the images \citep{Horry2020}.
As the developed custom augmentations did not work, this could pose an interesting research direction for manually designing augmentations for OOD detection.
\par
\textbf{Effectiveness of DINO pretraining depends on the pathologies.} 
In the second setting, DINO pretraining on CheXpert achieved slightly better results than the supervised baseline when fine-tuned (see Table \ref{table:finetuning-supervised-new}).
Fracture represents a low prevalence class in the CheXpert dataset (3.9\% positive occurrences) and pretrained representations seem to slightly enhance the detection of \textit{Fracture} as an OOD class.
This is consistent with recent findings of Van der Sluijs et al. \citep{Vandersluijs2023}, which show that pretrained visual representations can improve the performance in classifying low-prevalence classes compared to their supervised counterparts.
Still, the state-of-the-art performance (SOTA) of ODIN could not be reached (see Figure \ref{fig:supervised-vs-unsupervised}).
The authors in \citep{Berger2021} conduct an ablation study and show that perturbation and not the temperature scaling leads to the high performance of ODIN on chest X-rays.
%The ineffectiveness of temperature scaling was also observed in this thesis (see fig \ref{fig:auroc-vs-temp}).
Incorporating perturbations into the proposed unsupervised OOD detection framework could be interesting.
% Further, it is unclear if the results of ODIN are reproducible to other datasets.
% Berger et al. did not use OOD samples from the CheXpert dataset
% The results of ODIN should be reproduced on other pathologies and it should be evaluated if the perturbation is also effective for other pathologies and datasets.
% Applying generalized ODIN (GODIN) \citep{Hsu2020} could also be a promising research direction.
% The proposed method is similar to ODIN, e.g. input preprocessing is also applied to the images \citep{Hsu2020}, but it does not require access to OOD samples.
% \sout{However, ODIN assumes access to the OOD class labels that poses a limitation in the real world. 
% A potential way to overcome this issue would be to apply generalized ODIN (GODIN) \citep{Hsu2020}.
% The proposed method is similar to ODIN, e.g. input preprocessing is also applied to the images \citep{Hsu2020}.
% In contrast to ODIN it does not require access to OOD samples and a comparison between the performance of GODIN and ODIN on chest-X-ray OOD detection could be insightful.}
\\
No satisfactory results were achieved for the third setting.
The proposed ID-OOD split of the third setting in \citep{Berger2021} is questionable because \textit{Pneumonia} is a type of \textit{Lung Opacity} \citep{Hansell2008} and although no overlap was assumed between the ID and OOD class labels, the classes are very similar. 
Therefore, the trained models might confuse the two classes which degrades the OOD performance. 
Another classifier based approach would be to learn the sub labels of the super ID class to better separate it from the OOD class, still requiring label knowledge.
Outlier exposure could also be applied to setting 3, which was successful in setting 1.
Instead of \textit{Pneumonia}, classes like \textit{Cardiomegaly} or \textit{Pneumothorax} should be used as OOD classes.
%\sout{Similar to setting two, applying GODIN could also be a promising research direction.}
\par
\textbf{Limitations of proposed methods and dataset.}
The experiments were performed on a limited scope of the larger problem.
In particular, the second and third setting contain at most two ID classes and two OOD classes.
The findings should be evaluated on different chest X-ray datasets, such as the NIH chest X-ray dataset \citep{Wang2017} and on the ID-OOD splits developed in \citep{Cao2020} to ensure validity and generalizability.
For example, an ID-OOD split for novel disease detection developed in \citep{Cao2020} contains ten ID classes and four OOD classes.
For a more comprehensive evaluation, it is necessary to replicate the results on larger dataset splits.
To ensure consistency with \citep{Berger2021}, it was assumed that exactly one pathology occurs in each image.
In reality, this is not a plausible assumption as the occurrence of pathologies is not mutually exclusive.
Therefore, the results of this work should also be evaluated on dataset splits with a multi-label ID dataset and OOD detection methods should be modified to handle the multi-label case.
For unsupervised OOD detection with feature similarity as anomaly score this would not make a difference and the same approach could be applied.
However, for supervised OOD detection, the methods must be modified to handle the multi-label case.
Hendrycks et al. \citep{Hendrycks2022} recently proposed a method and use the logistic sigmoid function for multi-label classification.
Instead of MSP they compute the maximum unnormalized logit as an anomaly score.
% In addition, the resolution of the processed images is low. 
% Small details of local textures might be missed and affect the pathology detection. 
% Existing research methods could be applied to integrate images of higher resolution [SOURCES suchen].
\par
\textbf{Label Inconsistency.}
CheXpert labels were used as ground truth labels for the experiments.
However, the labels are noisy as they are extracted by NLP methods (see \ref{section: dataset}) and are also assigned uncertainty labels.
In addition, the label quality might be inconsistent across the different pathologies.
Abdalla et al. \citep{Abdalla2023} examine the label quality of the CheXpert dataset in a recent study.
They evaluate the consistency of the official gold standard CheXpert labels and conclude that the label noise deviates by pathology \citep{Abdalla2023}.
The gold standard labels are based on the majority vote of five radiologists randomly chosen from a pool of eight radiologists \citep{Irvin2019}.
The authors suppose, that the quality of the gold label set depends on the choice of the radiologists and randomly sample from the pool of eight radiologists to evaluate the agreement of all possible pairings \citep{Abdalla2023}. 
While \textit{Pneumothorax} and \textit{Lung Opacity} show high label consistency, the median agreement between the radiologist's labels for the \textit{Fracture} class is merely 50\% \citep{Abdalla2023}.
Actual performances may therefore be lower than the reported performances in this thesis.
Nguyen et al. \citep{Nguyen2022} released the dataset VinDr-CXR which contains 18,000 chest X-ray images that a group of 17 radiologists manually annotated.
One way to deal with the label inconsistency could be to pretrain the DINO models on CheXpert and then fine-tune on the VinDr-CXR dataset to evaluate the consistency across reported performances.
\par
\textbf{Future Work.}
In addition to images, radiology reports are another source of information and radiologists' judgments are often based on the combination of both. 
Datasets such as MIMIC-CXR \citep{Johnson2019} provide images and radiology reports. 
Multimodal SSL techniques such as CLIP \citep{Radford2021} can combine text and image data, and have recently been shown to be powerful OOD detectors in a large cohort of datasets \citep{Michels2023}.
Tiu et al. \citep{Tiu2022} have already applied CLIP to the classification of chest X-ray pathologies.
They train a CLIP model on the MIMIC-CXR dataset that learns to match the chest X-ray images with their corresponding radiology reports \citep{Tiu2022}.
Importantly, they do not use the labels of the images for training, but rather leverage the available text reports \citep{Tiu2022}.
This could mitigate the problem of noisy labels and label inconsistency and could be a promising research direction for future work on OOD detection of chest X-rays.
Furthermore, as DINO has already produced promising results, one could also apply the all-purpose feature extractor DINOv2 \citep{Oquab2023} to OOD detection of chest X-ray images. 
Further research should also focus on exploring how the global crop scale of the DINO learning objective relates to performance.
A larger global crop scale could be beneficial for chest X-rays, as otherwise small pathology regions might be missed.
Finally, ablation studies should be performed to investigate the effect of the ViT patch size on the OOD detection performance of X-ray images.
Smaller patch sizes may be advisable, especially for small textural incongruities on chest X-rays.