\section{Methods}
% Vision Transformers (ViTs) form the backbone of all models that will be trained in this thesis.
% They transfer the concept of text transformers to the image domain.
% SimCLR and DINO are SSL learning objectives to extract ViT-representations used for further downstream tasks, including out-of-distribution detection [SOURCE].
% The features are used for score based OOD detection methods. 
% We can use the features only for unsupervised ood detection and if label knowledge and further fine-tuning is assumed supervised ood detection methods can be applied.
% We can also inject further ood samples (Outlier exposure) if only one in-distribution class is available. 
% Custom augmentations are also applied to simulate certain pathology characteristics that are OOD samples.
\subsection{Out-of-Distribution Detection}
\label{section: Out-of-Distribution Detection}
\subsubsection{General Task}
Out-of-distribution detection is a method to filter unsuitable inputs that are not part of the training distribution \citep{Yang2021}.
Unsuitable input is considered out-of-distribution and reliable detection of such samples is indispensable for safety reasons \citep{Yang2021}.
Because the definition of the OOD dataset is task-oriented, it is common to distinguish between near OOD and far OOD \citep{Winkens2020}.
In the first case, ID and OOD datasets are (semantically) similar and in the second case, they are considered "unrelated" \citep{Winkens2020}.
It is clear from the definition that near OOD tends to be more complex than far OOD.
OOD detection is generally challenging because modeling every potential distribution is impossible \citep{Salehi2022}.
Fang et al. \citep{Fang2023} explore the theoretical limits of OOD detection.
They conclude that one cannot expect a universally working algorithm and that different methods must be applied for different use cases \citep{Fang2023}.
%Fort et al. relate the definition to a meaningful practical example: far OOD is the detection of house numbers (SVHN dataset) as OOD samples in a classifier trained on CIFAR-10 \ .
\par
Yang et al. \citep{Yang2021} develop a taxonomy for OOD detection methods and divide them into density-based, distance-based, reconstruction-based and score-based methods.
Density-based methods estimate the probability density function from the training data and assign a low probability to OOD samples.
Distance-based methods assume that OOD samples should be distant to the training data in some specified space.
The distance between the test sample (either ID or OOD) and a class centroid or prototype of the in-distribution is used to detect OOD samples.
Reconstruction-based methods apply autoencoder architectures for OOD detection.
Autoencoders are unsupervised encoder-decoder frameworks introduced in \citep{Rumelhart1986} and are trained to reconstruct the input.
The reconstruction loss is a distance measure between the input sample and the output of the decoder \citep{Baldi2012, Bank2020}.
For OOD detection, autoencoders are trained on ID samples, and it is assumed that the reconstruction fails for unseen OOD samples resulting in higher losses \citep{Denouden2018}.
Score-based methods typically rely on the prediction of a neural network and assign a confidence score to each test sample \citep{Yang2021}.
% measure the distance between the test sample and the training data and assign a low distance to in-distribution samples \citep{Yang2021}.
% Many machine learning models produce overconfident prediction and calibration is needed to mitigate potential failures [SOURCES].
%\textcolor{red}{erg√§nzen}
In this thesis, the focus is on score-based and distance-based OOD detection.
However, the distance is sometimes interpreted as a score and thus, the term "score" is used to refer to a distance measure.
\subsubsection{Supervised Out-of-Distribution Detection}
In supervised OOD detection, a common baseline is the \textit{Maximum Softmax Probability (MSP)} of a given in-distribution classifier \citep{Hendrycks2016a}.
The observation that a classifier assigns higher softmax probabilities to ID samples compared to OOD samples motivates using the MSP as a confidence score for OOD detection \citep{Hendrycks2016a}.
%It is based on the observation, that a pretrained classifier typically assigns higher softmax probabilities to ID samples compared to OOD samples \citep{Hendrycks2016a}.
%This motivates using the maximum softmax probability as a confidence score for OOD detection \citep{Hendrycks2016a}.
Given $C$ classes and a test sample $x'$, the MSP score is calculated as 
\begin{align}
	S_{MSP}(x')=\max_{i=1,\dots,C}p(y_i|x').
\end{align}	
\par
Although the MSP is simple and effective, it has drawbacks.
The authors in \citep{Lee2018} argue that the MSP is a confidence score that can produce high confidence for OOD samples, which is undesirable.
Instead, they suggest using the Mahalanobis distance as a score based on the representation $g(x)$ of a pretrained classifier with the input $x$ \citep{Lee2018}.
Let the labels $y_i$ of the training samples $\{(x_1,y_1),\dots, (x_N,y_N)\}$, $y_i \in \{1,\dots,C\}$ be known.
The authors fit a class conditional Gaussian to the representations of each class $c$ \citep{Lee2018}.
The estimates for the class means $\mu_c$ and for the collective covariance $\Sigma$ of the Gaussian are computed as \citep{Lee2018}
\begin{align}
	\hat{\mu}_c = \frac{1}{N_c}\sum_{i:y_i=c}g(x_i), \hspace{8pt} \hat{\Sigma} = \frac{1}{N}\sum_{c=1}^{C}\sum_{i: y_i=c}(g(x_i)-\hat{\mu}_c)(g(x_i)-\hat{\mu}_c)^T.
\end{align}
$N_c$ is the number of samples in class $c$ and $N$ is the number of training samples \citep{Lee2018}.
With estimated per class mean $\hat{\mu}_c$ and estimated covariance $\hat{\Sigma}$, the Mahalanobis distance of a test sample $x'$ is used as OOD score and calculated as \citep{Lee2018}
\begin{align}
	S_{MD}(x') =  \max_c -(g(x')-\hat{\mu}_c)^T\hat{\Sigma}^{-1}(g(x')-\hat{\mu}_c).
\end{align}
In the thesis, the Mahahalanobis distance is calculated in the feature space of the ViT encoder and in the penultimate layer of the classifier as suggested by \citep{Lee2018, Michels2023}.
\par
The ODIN method (Out-of-DIstribution detector for Neural networks) is an extension of the MSP score and is also based on a pretrained neural network $f(x)$ \citep{Liang2018}.
The authors suggest temperature scaling and input perturbations to improve the OOD detection performance of the MSP score \citep{Liang2018}.
First, assume a temperature scaling factor of $\tau$ \citep{Liang2018}.
A temperature scaled softmax score is obtained by \citep{Liang2018}
\begin{align}
	S_i(x,\tau) = \frac{\exp(f_i(x)/\tau)}{\sum_{j=1}^{C}\exp(f_j(x)/\tau)}, \forall i=1,\dots,C.
\end{align}
The temperature is set to $\tau_{tr}=1$ \citep{Liang2018} during training.
The second component of ODIN is adding a preprocessing step for each test sample $x'$.
As preprocessing, small perturbations are added to the input image $x'$ \citep{Liang2018}.
A gradient step is performed on the test input, resulting in a perturbed input $\tilde{x}'$ \citep{Liang2018}: 
\begin{align}
	\tilde{x}' = x' - \epsilon \text{sign}(-\nabla_x \log \text{max}_i  S_i(x',\tau_{tr})).
\end{align}
The parameter $\epsilon$ is the perturbation magnitude \citep{Liang2018}.
Then, the perturbed input $\tilde{x}'$ is used to calculate the confidence score \citep{Liang2018}:
\begin{align}
	S_{ODIN}(x,\tau) = \max_i S_i(\tilde{x}',\tau).
\end{align}
The temperature and perturbation size are tuned on validation data in the original paper and access to OOD samples is assumed \citep{Liang2018,Hsu2020}. 
Although \citep{Liang2018} argue that the parameters are insensitive to the tuning data and thus transferable, this is different from MSP and Mahalanobis distance which do not require access to OOD samples.
\subsubsection{Unsupervised Out-of-Distribution Detection}
When labels are not available, unsupervised methods can still be used to compute OOD scores.
Suppose that $g(x)$ is the representation of an input $x$.
Similar to \citep{Michels2023,Sun2022}, an OOD score can be constructed by applying the cosine similarity to a set of $k$-nearest neighbors.
Let $k$ be the number of nearest neighbors.
The training embeddings are denoted as $\{g(x_1),\dots,g(x_N)\}$ and the set of the $k$-nearest neighbors of a test sample $x'$ is denoted as $N_{x'} = \{g(x_{i_1}),\dots,g(x_{i_k})\}$, where $i_1,\dots,i_k$ are the corresponding $k$ training set indices.
The OOD score is calculated as \citep{Michels2023,Sun2022}
\begin{align}
	S_{NN}(x',k) = \frac{1}{k}\sum_{i=1}^k \text{sim}(x',N_{x'}(i)).
	\label{equation:knn-score}
\end{align}
The cosine similarity is used as the similarity measure, i.e. $\text{sim}(x,y) = \frac{x\cdot y}{\Vert x \Vert \Vert y \Vert}$ and in the case of $k=1$, the score is equivalent to the cosine similarity between the test sample and its nearest neighbor in \citep{Michels2023}.
\subsubsection{Outlier Exposure}
Outlier exposure is a method in OOD detection that exposes a proportion of known outliers to the training data \citep{Hendrycks2018}.
Recently, pretrained ViTs applied to outlier exposure have been shown to accelerate OOD detection performance when only a few examples per class are available \citep{Fort2021}.
The authors use a pretrained ViT encoder as a feature extractor and fine-tune a linear classifier on top of the features.
Suppose that $I$ in-distribution and $O$ out-of-distribution classes are available \citep{Fort2021}.
The authors distinguish between two settings \citep{Fort2021}.
In the first setting, the OOD class labels are ignored and the pretrained ViT encoder with linear classifier is fine-tuned on $I+1$ classes \citep{Fort2021,Thulasidasan2021}.
Since the actual fine-grained OOD classes are essentially unknown, this thesis refers to this scenario as \textit{unsupervised outlier exposure}.
In contrast, the second setting assumes that the (fine-grained) labels are known for all exposed outlier samples \citep{Fort2021,Roy2021}.
The pretrained feature extractor with a linear classifier on top is fine-tuned on $I+O$ classes \citep{Fort2021}.
In this case, the term used is \textit{supervised outlier exposure}.
In both supervised and unsupervised outlier exposure, the OOD score for a test input $x'$ is computed as the sum over the class probability of the $I$ in-distribution classes \citep{Fort2021}:
\begin{align}
	S_{OE}(x') = \sum_{i=1}^{I}p(y=c|x') 
\end{align}
\subsubsection{OOD Detection on Chest X-ray Images}
\label{section: OOD detection on Chest X-ray Images}
Detecting erroneous inputs to a machine learning model is especially important in the medical domain.
In general, OOD detection on chest X-ray images is a challenging task because variations between different pathologies are often nuanced \citep{Salehi2022}.
Detecting OOD samples is directly connected to the usability of the model and to the safety of the patient.
The authors in \citep{Cohen2019} demonstrate the former and develop \textit{Chester} as a disease prediction system for chest X-rays, where an integral pre-processing part of their pipeline is the detection of OOD samples through a reconstruction loss \citep{Dumoulin2017}. 
Images that are too different from the training distribution are rejected \citep{Cohen2019} and predictions are made only on samples within the "model's competence".
\par
A systematic evaluation of OOD detection on chest X-rays has recently been explored in \citep{Cao2020}.
The authors of \citep{Cao2020} propose a large cohort of methods to detect OOD samples on frontal views of the ChestX-ray8 \citep{Wang2017} dataset.
They assume a training dataset with labeled in-distribution examples, a validation set and a test set with ID and OOD samples \citep{Cao2020}.
In total, 22 different models are evaluated on three classes of OOD detection methods \citep{Cao2020}.
The three different classes of OOD detection methods are grouped into data-only methods (KNN-1 and KNN-8), classifier-based methods (including but not limited to MSP, Mahalanobis distance and ODIN) and auxiliary methods (which use training objectives other than classification, such as image reconstruction loss) \citep{Cao2020}.
\par
OOD samples are defined as unrelated images (use case 1), incorrectly acquired samples (use case 2), or images with unseen pathologies (use case 3) \citep{Cao2020}.
For the first two use cases, the authors obtain satisfactory results above the random guess of 50\% AUROC \citep{Cao2020}.
However, for the third use case, which is the focus of this thesis, all evaluated methods score close to 50\% AUROC and the authors caution users of diagnostic tools not to trust every output \citep{Cao2020}.
\par
Berger et al. focus on the third use case \citep{Berger2021}, applying classifier-based methods and evaluating performance on two ID-OOD splits (see section \ref{section: ID-OOD splits}) of the CheXpert dataset \citep{Irvin2019} with different pathologies \citep{Berger2021}.
They use a WideResNet \citep{Zagoruyko2016} with a depth factor of 100 and a width factor of 2 and train a classifier to distinguish the classes of each in-distribution \citep{Berger2021}.
For OOD detection, they use the MSP score, the Mahalanobis distance and ODIN \citep{Berger2021}, among others.
It is important to note that the authors did not have access to OOD samples for ODIN and used the parameters of the original paper \citep{Liang2018}, tuned on the CIFAR-10 dataset \citep{Krizhevsky2009b}.
They identify ODIN as the best performing method, with an AUROC of 84.1\% on the first ID-OOD split (\textit{Cardiomegaly} and \textit{Pneumothorax} as ID and \textit{Fracture} as OOD) and 86.5\% on the second split (\textit{Lung Opacity} and \textit{Pleural Effusion} as ID and \textit{Pneumonia} and \textit{Fracture} as OOD) \citep{Berger2021}.
The results in \citep{Berger2021} are used as the supervised baseline.
\par
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Introductory stuff about SSL.
\subsection{Vision Transformers}
\label{section: Vision Transformers}
Vision Transformers are the reformulation of the Transformer architecture \cite{Vaswani2017} to images.
Originally, sequences of natural language are split into tokens and then processed by the text Transformer \citep{Vaswani2017}.  
Similarly, ViTs process images as a fixed number of flattened image patches \citep{Dosovitskiy2020}.
Although the Transformer is designed as an encoder-decoder architecture, the Transformer architecture is introduced in this thesis with an emphasis on the encoder part, since only the encoder is used within the ViT architecture.
Then, minor adaptions needed to accept images instead of text as input are presented.
\par
The input is assumed to be a sequence $(x_1,\dots,x_n)$, which is mapped to a continuous representation $z=(z_1,\dots,z_n)$ by the encoder \citep{Vaswani2017}.
The encoder is an encoder \textit{stack} and consists of a stack of $L$ layers \citep{Vaswani2017}.
The outputs of each layer in the encoder (including the embedding layers) are of size $d_{model}$ and at the bottom of the encoder, the input is embedded into a $d_{model}$ dimensional embedding space.
Then the positional encodings are added. \citep{Vaswani2017}.
Since neither recurrent connections nor convolution are part of the Transformer architecture, positional encodings are necessary to force the model to learn relative and absolute location information within the sequence \citep{Vaswani2017}.
%The authors experimented with learning positional encodings \citep{Gehring2018}, but decided to stay with sinusoidal positional encodings, as it theoretically allows the model to generalize to larger, unseen sequence lengths not seen in training \citep{Vaswani2017}.
The positional encodings depend on both the position $pos$ of the element within the sequence and on the parity of the dimension $i$ of the i-th component of the embedding vector.
This means that depending on whether $i$ is an odd or even number, the following functions are applied to retrieve a positional encoding \citep{Vaswani2017}:
\begin{equation}
	PE(pos, i) = 
	\begin{cases}
		sin(\frac{pos}{10000^{2i / d_{model}}}), i \text{ is even} \\
		cos(\frac{pos}{10000^{2i / d_{model}}}), \text{ else}.
	\end{cases}
	\label{equation:positional-encoding}
\end{equation}
\par
Two sublayers further process the input (embedding + positional encoding) and each sublayer is wrapped with a residual connection \citep{He2016}.
Then, layer normalization \citep{Ba2016} is applied on x + Sublayer(x), while Sublayer(x) is the output of the respective sublayer \citep{Vaswani2017}.
More precisely, applying a normalization layer to a vector $u\in \mathbb{R}^d$ corresponds to the following computation \citep{Ba2016,Xiong2020}:
\begin{align}
	\text{LayerNorm}(u) = \gamma \frac{u-\mu}{\sigma}+\beta,\\
	\mu = \frac{1}{d}\sum_{k=1}^{d}u_k, \sigma^2 = \frac{1}{d}\sum_{k=1}^{d}(u_k - \mu)^2.
\end{align}
The mean and standard deviation of $u$ are denoted as $\mu$ and $\sigma$ \citep{Xiong2020}.
A scale parameter $\gamma$ and a bias vector $\beta$ are also included to shift the vector $u$ \citep{Xiong2020}.
\par
The first sublayer of the encoder architecture is a multi-head attention layer.   
These layers are one of the main components of the Transformer architecture and require the concept of attention \citep{Vaswani2017}.
Attention mechanisms are functions that map three input vectors $q,k,v$ called query, keys and values to an output \citep{Vaswani2017}.
This output is a linear combination of the values $v$ where the specific weights for each component $v_i$ are obtained by a similarity function between the query and the corresponding keys \citep{Vaswani2017}.
The specific attention function used is called "Scaled Dot-Product Attention" and is implemented by \citep{Vaswani2017}
\begin{align}
    \text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V.
	\label{equation:attention}
\end{align}
The three matrices $Q,K,V$ are a stack of the vectors $q,k,v$, which are processed together and are called queries, keys and values similar to before.
Each vector component of $Q$ and $K$ is $d_k$ dimensional and the matrix $V$ is a stack of $d_v$ dimensional vectors \citep{Vaswani2017}.
The Equation \ref{equation:attention} is a specific adaptation of dot-product attention with a scaling factor being the reciprocal of the square root of the dimension $d_k$, which helps to mitigate the risk of vanishing softmax gradients for larger values of $d_k$ \citep{Vaswani2017}.
The dot product is realized through the matrix multiplication $QK^T$.
A multi-head attention layer now builds upon the mentioned attention mechanism by concatenating the output of $h$ different attention functions.
Queries, keys and values are mapped to $h$ distinct subspaces with weight matrices $W_i^Q$, $W_i^K$, $W_i^V$ where $i=1,\dots,h$ \citep{Vaswani2017}.
Multi-head attention allows the model to mutually attend to the knowledge of several representational subspaces at different positions \citep{Vaswani2017}:
\begin{align}
	\text{MultiHead}(Q,K,V) &= \text{Concat}(\text{head}_1,\dots,\text{head}_h)W^O, \\
	\text{head}_i & = \text{Attention}(QW_i^Q,KW_i^K,VW_i^V), \\
	& W_{i}^{Q}, W_{i}^{k} \in \mathbb{R}^{d_{model}\times d_k} , W_i^V \in \mathbb{R}^{d_{model} \times d_v}, W^O \in \mathbb{R}^{hd_v \times d_{model}}.	
\end{align}
After concatenating the attention of each head, the outputs are projected back onto the model dimension $d_{model}$ with the matrix $W^O$ and passed to the second sublayer of the Transformer encoder block \citep{Vaswani2017}.
\par
The second sublayer is a fully connected neural network with two linear layers separated by a ReLU activation \citep{Vaswani2017}.
The output of this feed-forward network is also wrapped by a residual connection, followed by layer normalization, as explained above.
Both input and output dimensions are set to $d_{model}$ and the inner dimension (output dimension of the first layer and input dimension of the second layer) is set to $d_{ff}$ \citep{Vaswani2017}.
Thus, the second sublayer is implemented by the following formula:
\begin{equation}
	\text{FFN}(x) = \max(0, xW_1 + b_1)W_2+b_2. 
	\label{equation:FFN}
\end{equation}
In previous paragraphs, the input sequence of the Transformer was assumed to be a one-dimensional text sequence (i.e. a sentence).
The following paragraph introduces necessary architectural modifications to allow 2D images as inputs \citep{Dosovitskiy2020}.
\par
Assume a 2D image $x\in \mathbb{R}^{H\times W \times C }$ where $(H, W)$ is the shape (height $\times$ width) and C is the number of (color) channels of the image expressed as a 3D tensor \citep{Dosovitskiy2020}.
The image is now divided into image patches of size $(P, P)$ and the input tensor $x$ is flattened into a sequence of two-dimensional patches $x_p \in \mathbb{R}^{N \times (P^2C)}$ where the number of input patches is computed as $N = HW/P^2$ \citep{Dosovitskiy2020}.
Each patch is spatially flattened and concatenated across the color channels \citep{Dosovitskiy2020}.
The sequence of flattened image patches is projected to the constant model dimension $d_{model}$ with a linearly trainable patch embedding \citep{Dosovitskiy2020,Vaswani2017}.
Furthermore, a trainable \texttt{[class]} token is inserted at the first position of the sequence of the embedded patches, whose state at the last layer $L$ of the ViT architecture is the representation of the image $y$ \citep{Dosovitskiy2020}.
This approach is based on the BERT \texttt{[CLS]} token \citep{Devlin2018} and the output $y$ will serve as the feature vectors (representations) in this thesis.
\par
ViTs also use positional encodings similar to the standard Transformer architecture.
Unlike the fixed sinusoidal positional encodings mentioned in Equation \ref{equation:positional-encoding}, Dosovitskiy et al. \citep{Dosovitskiy2020} use trainable parameters to encode position into the image patch.
The authors hypothesize that because ViTs work with image patches rather than processing the information pixel-wise, the differences in how spatial information is enforced are minor \citep{Dosovitskiy2020}.
After the positional encoding is added, the sequence is passed to the Transformer encoder.
The architecture of the ViT is shown in Figure \ref{fig:vit-arch}.
\loadFigure{Figure:vit-arch}
\par
Depending on the number of layers $L$, the hidden model size $d_{model}$, the inner dimension $d_{ff}$ of the FFN network (see Equation \ref{equation:FFN}) and the number of heads $h$, Dosovitskiy et al. \citep{Dosovitskiy2020} define the three model variants ViT-Base (ViT-B), ViT-Large (ViT-L) and ViT-Huge (ViT-H) inspired by \citep{Devlin2018}.
The authors in \citep{Touvron2020} introduced a smaller variant DeiT-S with only six heads, but with the same dimension \textit{per} head as ViT-B.
This thesis uses the smaller DeiT-S variant \citep{Touvron2020} and the base configuration ViT-B with patch size 16.
As in \citep{Caron2021}, the smaller variant is referred to as ViT-S/16 or simply ViT-S.
ViT-S and ViT-B configurations are detailed in Table \ref{table:vit-model-variants}.
\loadTable{Table:vit-model-variants}
\par
While Vision Transformers serve as the backbone model, different projection heads are trained on top of the backbone. 
After training, the projection head is discarded and the features of the ViT are used to detect the OOD classes with the methods described in section \ref{section: Out-of-Distribution Detection}.
The projection heads used are described in the next section.
\subsection{Self-Supervised Learning}
\label{section: SSL}
\subsubsection{SimCLR}
\label{section: SimCLR}
SimCLR is a training schema for learning contrastive visual representations \citep{Chen2020}.
Contrastive learning is a learning objective that encourages augmented versions of the same training samples to be close together in an embedding space while simultaneously repelling those embeddings from different samples \citep{Jaiswal2020}.
SimCLR is self-supervised, i.e. no label knowledge is assumed \citep{Chen2020}.
The framework consists of four main parts and the notation of the original paper is adapted \citep{Chen2020}.
\par
In the first component, data augmentation is applied to the input image, resulting in two randomly altered image views $\tilde{x}_i$ and $\tilde{x}_j$ that form a positive pair (i.e. an instance of the same class) and are passed to the encoder network \citep{Chen2020}.
The original SimCLR image augmentations consist of \textit{random resized cropping}, \textit{random color distortions} and \textit{random Gaussian blur} \citep{Chen2020}.
\loadFigure{Figure:simclr-arch}
\par
In the second component, both views $\tilde{x}_i$ and $\tilde{x}_j$ are passed to an encoder network $f(\cdot)$ that returns two representations $h_i$ and $h_j$ \citep{Chen2020}.
The network choice is flexible and in this thesis, a ViT encoder is used as an encoder network.
Specifically, applying the ViT to the augmented view $\tilde{x}_i$ results in $h_i = f(\tilde{x}_i) = \text{ViT}(\tilde{x}_i)$ with $h_i \in \mathbb{R}^{d_{model}}$ being the output of layer $L$ restricted to the \texttt{[class]} token (see \ref{section: Vision Transformers}) \citep{Chen2020}.
\par
Then, both representations are projected with a feed-forward network from the embedding space $\mathbb{R}^{d_{model}}$ to a space where the loss is applied \citep{Chen2020}.
Similar to before, this projection head $g(\cdot)$ consists of a two-layer neural network with inner dimensions that are equal to the embedding size (e.g. for ViT-S $d=384$), output dimension of 128 and with a ReLU activation in-between \citep{Chen2020} and yields a projected vector $z_i = \max(0, h_iW_1 + b_1)W_2+b_2, z_i \in \mathbb{R}^{128}$.
\par 
The last component of the SimCLR module is a contrastive loss function that is applied to the output of the projection head.
Assume $S=\{{\tilde{x}}_k\}$ and positive pairs $\tilde{x}_i, \tilde{x}_j \in S$ \citep{Chen2020}.
The contrastive prediction task is defined as determining the correct $\tilde{x}_j$ in $S\setminus \tilde{x}_i$ for a given $\tilde{x}_i$ \citep{Chen2020}.
The loss is applied to a batch of images in the following way \citep{Chen2020}:
If $N$ is the size of the batch, two augmented views are generated for each image, for a total of $2N$ views \citep{Chen2020}.
The authors then contrast a positive pair against all the other $2N-2$ augmented batch elements \citep{Chen2020}.
Specifically, all other augmented views are interpreted as negative pairs \citep{Chen2020}.
The resulting loss function has also been included in previous work by \citep{Sohn2016,Wu2018,Oord2018}, is defined for a positive pair $(i,j)$ of augmented examples and has been termed \textit{NT-Xent} by \citep{Chen2017}:
\begin{align}
	\ell_{i,j} = -\log\frac{\exp(\text{sim}(z_i,z_j)/\tau)}{\sum_{k=1}^{2N} \mathbbm{1}_{[k \neq i]}\exp(\text{sim}(z_i,z_k)/\tau)}, \tag{NT-Xent}\\
	\text{sim}(u,v) = \frac{u\cdot v}{\Vert u \Vert \Vert v \Vert}, \\
	\mathbbm{1}_{[k \neq i]} = \begin{cases}
		1, \text{ if } k \neq i \\
		0, \text{ else}.
	\end{cases}
\end{align}
The parameter $\tau$ refers to the temperature and the similarity function used is the cosine similarity \citep{Chen2020}.
The final loss is calculated over all positive tuples $(i, j)$ and also $(j, i)$ \citep{Chen2020}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{DINO}
\paragraph{Teacher and Student Network} \mbox{} \\
\\
DINO is an abbreviation for "Distilled Knowledge with No Labels" and both the notation and terminology of the following subsection are adapted from \citep{Caron2021}.
The authors base their training method on knowledge distillation (KD).
KD was introduced in \citep{Hinton2015} and refers to the transfer of prediction capabilities from a large model to a smaller (compressed) one.
The bigger model is trained on a large amount of data and knowledge is injected into the smaller model, which is forced to replicate the output of the larger model \citep{Hinton2015,Caron2021}.
Often the larger model is referred to as the \textit{teacher network}, while the smaller model is called the \textit{student network} \citep{Caron2021}. 
A major difference between the DINO paradigm and the original KD concept is that no pretrained teacher network is assumed \citep{Caron2021}.
Both teacher and student networks are parametrized by a weight vector $\theta_t$ and $\theta_s$ respectively.
The output $g_{\theta_s}$ of the student network is trained to equal the output $g_{\theta_t}$ of the teacher network \citep{Caron2021}.
\par
Multiple views of an image $x$ are passed through each network, given both student and teacher network.
The architectural choice of each network is discussed after giving an overview of the training objective.
The output of both teacher- and student network is normalized with a softmax function, which results in two $D$-dimensional probability distributions $P_s$ and $P_t$ \citep{Caron2021}:
\begin{align}
	P_{s}(x)^{(i)} = \frac{\exp(g_{\theta_{s}}(x)^{(i)}/\tau_{s})}{\sum_{d=1}^{D}\exp(g_{\theta_{s}}(x)^{(d)}/\tau_{s})}, \hspace{8pt} i=1,\dots,D, \\
	P_{t}(x)^{(i)} = \frac{\exp(g_{\theta_{t}}(x)^{(i)}/\tau_{t})}{\sum_{d=1}^{D}\exp(g_{\theta_{t}}(x)^{(d)}/\tau_{t})}, \hspace{8pt} i=1,\dots,D.
	\label{equation:dino-softmax}
\end{align}
The temperature $\tau$ controls the sharpness of the output distribution \citep{Caron2021} and larger values for $\tau$ tend to produce smoother distributions \citep{Hinton2015}.
The student-teacher network of the DINO head is depicted in Figure \ref{fig:dino-arch}.
\loadFigure{Figure:dino-arch}
\par
The normalized output $P_s$ and $P_t$ are now matched with the cross-entropy.
Let $H(a,b)$ denote the cross-entropy of two probability vectors $a$, $b$, i.e. $H(a,b)=-a \log b$ and further, assume a fixed teacher network parametrized by $\theta_t$ \citep{Caron2021}.
Both distributions are aligned by minimizing the cross-entropy of the fixed teacher distribution and the student distribution with respect to $P_s$ \citep{Caron2021}: 
\begin{align}
	\hat{\theta}_s = \min_{\theta_s}H(P_t(x),P_s(x)).
	\label{equation:ce-student-teacher}	
\end{align}
Equation \ref{equation:ce-student-teacher} is used as a base for the training loss and further self-supervised learning techniques are adopted to boost model performance \citep{Caron2021}.
\paragraph{Multi-crop Strategy} \mbox{} \\
\\
Analogous to the SimCLR architecture, different augmented views of the same image are generated and passed to the model.
While in the standard SimCLR setting two views are generated \citep{Chen2020}, the DINO head uses a \textit{set} $V$ of multiple views which typically includes more than two views \citep{Caron2021}.
\par
This multi-crop augmentation strategy creates two global views of full resolution and several lower resolution local views \citep{Caron2020, Caron2021}. 
The full resolution views embody large parts (e.g. more than 50\%) of the original image at resolution $224^2$ and the local views of resolution $96^2$ are cropped to a smaller size (e.g. maximum 50\% of the original image) \citep{Caron2021}.
The crop is passed to the student or the teacher network, depending on the view type.
While the student network receives all crops (global and local), the teacher network only processes global views \citep{Caron2021}.
According to the authors, this should enforce the model to gather information from small image patches (local information) and connect them to its global image interpretation \citep{Caron2021}.
Now, given the set of views $V$, the following, adapted training loss is minimized \citep{Caron2021}:
\begin{align}
	\min_{\theta_s}\sum_{x \in \{x_1^g, x_2^g\}} \hspace{5pt} \sum_{\substack{ x' \in V, \\ x\neq x'}} H(P_t(x),P_s(x')).
	\label{equation:dino-training-objective}
\end{align}
The outer sum collects both global views $x_i^g, i\in\{1,2\}$ that are passed to the teacher network and the inner sum ranges over all local views $x' \neq x, x' \in V $ that are passed to the student network \citep{Caron2021}.
If the teacher distribution is fixed, the student distribution that minimizes the loss minimizes the sum of the cross-entropies between all views seen by the student and the two global views processed by the teacher network.
All views are processed by the student network and are aligned to both global views.
%\par
\paragraph{Backbone and Projection Head} \mbox{} \\
\\
As explained before, both probability distributions $P_s$ and $P_t$ are the normalized output of the same network architectures $g_{\theta_s}$ and $g_{\theta_t}$ \citep{Caron2021}.
The networks $g$ are the composition of a backbone network $f$ with an MLP projection head $h$ \citep{Caron2021}.
The MLP projection head is similar to the one used in \citep{Caron2020}, which is based on the SimCLR projection head described in section \ref{section: SimCLR}.
The choice of the backbone network is flexible, but the authors primarily use a ViT architecture \citep{Dosovitskiy2020}, which is also the architectural choice in this thesis.
First, the backbone $f$ is applied, and the resulting features are passed to the first two layers of the projection head with an output dimension of 2048 and a Gaussian Error Linear Units (GELU) activation in-between \citep{Hendrycks2016b}:
\begin{equation}
\text{GELU}(x) = x \Phi(x).
\end{equation}
The cumulative distribution $\Phi(x)$ of the standard normal probability mass functions is used inside the GELU function, which is applied after each of the first two layers \citep{Hendrycks2016b}.
The last layer of the projection head does \textit{not} use a GELU activation and projects the features to a bottleneck dimension of 256 \citep{Caron2021}.
Then, the output is $\ell_2$-normalized ($x \leftarrow x/\Vert x \Vert_2$) so that each vector has unit norm \citep{Caron2021}.
Finally, the output is passed to a fully connected layer of dimension $D$ with weight normalization.
Weight normalization speeds up convergence by reformulating the weight vectors in a neural net with different parameters and was proposed in \citep{Salimans2016}.
The weights $w$ of a neural net are reparametrized to a new parameter vector $v$ of the same dimension as $w$ and with a scalar parameter $g$ \citep{Salimans2016}:
\begin{equation}
	w = g \frac{v}{\Vert v \Vert}.
\end{equation}
Both parameters are trainable and optimization is performed on $g$ and $v$ and not on $w$ \citep{Salimans2016}.
\paragraph{Optimization of both Networks} \mbox{} \\
\\
While both networks share the same architecture, student network and teacher network are updated differently. 
The parameter weights of the student network $\theta_s$ are updated by stochastic gradient descent with learning rate $\eta$ and fixed teacher network $\bar{P}_t$, i.e. \citep{Caron2021}:
\begin{align}
	\mathcal{L}  &= \sum_{x \in \{x_1^g, x_2^g\}} \hspace{5pt} \sum_{\substack{ x\prime \in V, \\ x\neq x\prime}} H(\bar P_t(x),P_s(x\prime)), \\ 
	\theta_s &\leftarrow \theta_s - \eta \nabla_{\theta_s} \mathcal{L}.
\end{align}
New parameter weights are obtained for the teacher network by adding the weighted convex combination $(1-\lambda)\theta_s$ to the current value $\lambda\theta_t$.
The parameter weights of $g_{\theta_t}$ are obtained by calculating the following exponential moving average (EMA) \citep{Grill2020,Caron2021}: 
\begin{align}
	\theta_t \leftarrow \lambda \theta_t + (1-\lambda) \theta_s.
	\label{equation:dino-momentum-encoder}
\end{align}
The momentum parameter $\lambda$ in Equation \ref{equation:dino-momentum-encoder} is set with a cosine schedule \citep{Grill2020}.
To mitigate the risk of a complete model collapse, which essentially means that all feature vectors are mapped to a constant point \citep{Jing2022}, the authors also apply centering and sharpening on the output of the teacher network \citep{Caron2021}.
They experimentally confirm that the centering technique is necessary to prevent one dimension from dominating the output \citep{Caron2021}.
Centering is achieved by adding a bias term $c$ to the output of the teacher network and $c$ is improved by an exponential moving average with a parameter $m$ that averages between the previous $c$ and the average teacher output over the current batch with batch size $B$ \citep{Caron2021}:
\begin{align}
	c & \leftarrow mc + (1-m) \frac{1}{B}\sum_{i=1}^B g_{\theta_t}(x_i), \\
	& g_t(x) \leftarrow g_{\theta_t}(x) + c.
	\label{equation:dino-centering}
\end{align}
To counteract the effect of centering, which can lead to a collapse to a uniform distribution, the authors apply sharpening to the teacher output \citep{Caron2021}.
The sharpening of the teacher output is achieved by using a low value for the temperature parameter $\tau_t$, when applying the softmax normalization in Equation \ref{equation:dino-softmax} \citep{Caron2021}.
%As already mentioned, lower temperature values tend to sharpen the probability distribution \textcolor{red}{\citep{Hinton2015}}.
%The student-teacher network of the DINO head is depicted in Figure \ref{fig:dino-arch}.
%\loadFigure{Figure:dino-arch}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsubsection{Supervised Contrastive Learning}
% \label{section: supervised-contrastive-learning}
% Previously explained projection heads did \textit{not} assume any label information and were therefore self-supervised.
% The projection head, that is applied in supervised contrastive learning assumes label knowledge and therefore embeddings of the same class may be pushed closer together in the embedding space \citep{Khosla2020}.
% Similar to the prior described architectures, the authors apply data augmentation, an encoder network (backbone) and a contrastive head that integrates supervision into the loss function \citep{Khosla2020}.
% \par
% As data augmentation module, the authors ablate four different augmentation strategies including RandAugment \citep{Cubuk2019a}, AutoAugment \citep{Cubuk2019b}, SimAugment \citep{Chen2020} and Stacked RandAugment \citep{Tian2020}.
% SimAugment refers to an augmentation strategy which takes the SimCLR augmentations and adds image warping before the Gaussian Blur which boosts performance \citep{Khosla2020}
% While there are observed differences in performance between the different augmentation strategies \citep{Khosla2020}, the results refer to the accuracy on ImageNet \citep{Deng2009}, and it is not clear how well they generalize to medical images, let alone chest X-rays.
% In this thesis, the adapted SimCLR augmentations described in \ref{section: SimCLR} are used for simplicity and to ensure consistency when comparing results.
% In addition to that, image warping is also omitted, as this may lead to a misinterpretation of the model due to unnatural pathology dimensions.
% For example, the heart could be warped to an unnatural size and lead to false positive classifications of cardiomegaly.
% \par
% Identical to before we will use a ViT as the backbone network.
% Two augmented samples are fed into the encoder network and the features are then $\ell_2$ normalized \citep{Khosla2020}.
% The features are passed to an MLP projection network.
% First, the features are passed through a fully connected layer with a hidden dimension of $d_{model}$ and a ReLU activation function and then, the output is linearly projected into a 128d space where the loss is applied \citep{Khosla2020}.
% \par
% Now the loss function is introduced.
% Notations and definitions are taken from \citep{Khosla2020}.
% Let $N$ be the batch size.
% As a starting point, consider $2N$ augmented image samples $\tilde{x}_l$ and their corresponding class labels $\tilde{y}_l$, which together form a set of $2N$ pairs $\tilde{X} = \{\tilde{x}_l, \tilde{y}_l\}_{l=1,\dots,2N}$ \citep{Khosla2020}.
% Let $i \in I:=\{1,\dots,2N\}$ denote the random index of a pair in $\tilde{X}$ and let $A(i):=I\setminus \{i\}$ \citep{Khosla2020}.
% Further, consider $P(i):=\{p\in A(i): \tilde{y}_p = \tilde{y}_i\}$ to be the set of all pairs that have the same class label (positives) as the pair at index $i$ \citep{Khosla2020}.
% Note that $i \not \in P(i)$ as $p$ needs to be an element of $A(i)$.   
% In addition, with $z_{l} := Projection(Encoder(\tilde{x}_l)) \in \mathbb{R}^{128}$, the authors define a suitable loss function $\mathcal{L}_{out}^{sup}$, that incorporates label information \citep{Khosla2020}:
% \begin{align}
% 	\mathcal{L}_{out}^{sup} &= \sum_{i\in I} -\frac{1}{\vert P(i) \vert} \sum_{p\in P(i)} \log \frac{\exp(z_i \cdot z_p/\tau)}{\sum_{a \in A(i)}\exp(z_i \cdot z_a/\tau)}
% 	\label{equation: superv-contrastive-loss}
% \end{align}
% The \textit{out} subscript indicates, that the summation over all positive examples in $P(i)$ is performed \textit{outside} the log function \citep{Khosla2020}.
% One can also define the loss function $\mathcal{L}_{in}^{sup}$, which is similar to the previously described loss in equation \ref{equation: superv-contrastive-loss}, but now the summation over positives is with\textit{in} the logarithmic function \citep{Khosla2020}.
% A comparison of the properties can be found in \citep{Khosla2020}, but for the purpose of this thesis, the higher performant loss function in equation \ref{equation: superv-contrastive-loss} is used as recommended by the authors.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Setup}
\label{section: experimental-setup}
\subsection{Dataset}
\subsubsection{General Description}
\label{section: dataset}
The CheXpert dataset is a collection of chest X-ray images developed in \citep{Irvin2019}.
The following subsection will describe the dataset and summarize the label extraction methods of radiology reports in more detail.
Further, the reasoning behind the choice of used dataset splits is explained. 
CheXpert is an extensive collection of frontal and lateral chest X-ray images with 224,316 images in total which were obtained from 65,240 patients.
\par
The dataset contains labels of twelve common chest- and lung pathologies, the control class \textit{No Finding} and the \textit{Support Device} class which adds up to 14 total classes \citep{Irvin2019}.
The 14 classes of the dataset were selected due to both common occurrence in the radiologic reports and clinical importance \citep{Irvin2019} and refer to the definitions in the glossary of terms for thoracic imaging \citep{Hansell2008}.
Figure \ref{fig:multilabel-struc} shows all classes and their hierarchical structure.
CheXpert is a multi-label dataset, meaning that multiple pathologies may be present in each image.
For each image, the pathologies are labeled as positive (1), negative (0) or uncertain (-1), where uncertainty labels express both contradictions in the report and also diagnostic uncertainty of the radiologist who has written the report \citep{Irvin2019}.
\par
Labels were extracted from free-text radiology reports in three stages.
In the first step, observations were extracted from the reports with the help of a hand-selected list of common phrases reviewed by different board-certified radiologists \citep{Irvin2019}.
The extracted observations are passed to a second stage.
This stage is split into a three steps pipeline, where observations are matched against pre-negation uncertainty rules first, then against negation rules and finally against post-negation uncertainty rules \citep{Irvin2019}.
Only if an observation does not match any of the rules a positive label is assigned \citep{Irvin2019}.
The rules are extracted through NLP techniques \citep{Irvin2019}.
In the last step, mentions are aggregated into a final label for each image and each class.
Observations with at least one positive labeled mention are assigned to an aggregated positive label.
An uncertainty label is assigned if there is no positive mention and at least one mention of uncertainty in the observations \citep{Irvin2019}.
Negative labels are only assigned if at least one mention label is negative \citep{Irvin2019}. 
If there is overall no mention of an observation, a \textit{blank} label is set as the label of the respective class \citep{Irvin2019}.
Medical imaging datasets differ from commonly used computer vision benchmarks like CIFAR-10 \citep{Krizhevsky2009a}, CIFAR-100 \citep{Krizhevsky2009b} or ImageNet \citep{Deng2009} in terms of appearance and variability of input images.
Chest X-ray images are grayscaled.
As only the upper body part is included, there is also low input variability.
Further, classifying the underlying pathologies is challenging for certified board radiologists and depends on the pathology.
On the CheXpert dataset, the miss rate (False Negative Rate) for classifying \textit{Consolidation} can range from 55\% to 52\% or 37\% depending on the human radiologist assessing the X-ray image \citep{Irvin2019}, where lower values indicate good performance.
On the \textit{Atelectasis} class, expert performance might be as high as 92\% precision (Positive Predictive Value) with higher values corresponding to better performance \citep{Irvin2019}.
In addition, a study has shown that performance on the same pathology can also vary between X-ray datasets \citep{Majkowska2020}.
\loadFigure{Figure:multilabel-struc}
\par
\subsubsection{ID and OOD Settings}
\label{section: ID-OOD splits}
The multi-label structure and the number of different classes of the CheXpert dataset allow the design of different ID and OOD split settings.
Two settings were developed for the CheXpert dataset in \citep{Berger2021} for novel disease detection.
In this thesis, a total of three settings are evaluated. 
No label overlap between ID and OOD classes is assumed for comparison with the results of \citep{Berger2021}.
In the first setting, the \textit{No Finding} class is the ID class and six pathologies are used as OOD classes.
Specifically, the model is trained on the \textit{No Finding} class and performance metrics are evaluated separately on each of the six classes.
The OOD classes are \textit{Cardiomegaly}, \textit{Fracture}, \textit{Lung Opacity}, \textit{Pleural Effusion}, \textit{Pneumothorax} and \textit{Support Devices}.
\loadTable{Table:settings-dataset-sizes}
\par
Since CheXpert labels are hierarchically structured \cite{Irvin2019}, the pathologies \textit{Atelectasis}, \textit{Edema}, \textit{Consolidation}, \textit{Lung Lesion} and \textit{Pneumonia} are subcases of \textit{Lung Opacity} (see Figure \ref{fig:multilabel-struc}).
Therefore, in the first setting, which contrasts the \textit{No Finding} class with the mentioned OOD classes, images of the mentioned subcases are also added to the \textit{Lung Opacity} class.
The OOD detection performance evaluated on \textit{Lung Opacity} is averaged over five subclasses and one superclass.
Further, the two different settings of \citep{Berger2021} are used.
The first one defines \textit{Cardiomegaly} and \textit{Pneumothorax} as ID pathologies and \textit{Fracture} as OOD class.
The other setting consists of the two in-distribution classes \textit{Lung Opacity} and \textit{Pleural Effusion}, which are evaluated against the two OOD pathologies \textit{Fracture} and \textit{Pneumonia}.
Settings and dataset sizes are summarized in Table \ref{table:settings-dataset-sizes}.
\subsubsection{Pathologies}
Due to the differences between CheXpert and the frequently used benchmarks, medical domain knowledge is advantageous and can result in better performance when designing the model architecture or the data augmentations for specialized medical imaging datasets.
In this paragraph, six different conditions are briefly discussed providing basic medical knowledge about the CheXpert labels contained in different ID-OOD settings in this master thesis.
The label names are taken from the CheXpert dataset \citep{Irvin2019} and adhere to the glossary \citep{Hansell2008} if mentioned there.
\loadFigure{Figure:pathologies}
\par
\textit{Cardiomegaly} is the first condition of interest.
It is a general term for heart enlargements, where the cardiothoracic ratio is often used to measure the severity of the increase in size.
In \citep{Dahnert2011} the ratio is defined as the ratio between heart size and thorax width. 
Values between 0.45 - 0.55 are marked as mild \textit{Cardiomegaly} and values above 0.55 are considered as severe cases of \textit{Cardiomegaly} \citep{Dahnert2011}.
\textit{Pneumothorax} is a medical condition related to the pulmonary pleura and is used as an ID pathology.
It is defined as a loss of negative pressure in the pleural space, which can lead to a lung collapse \citep{Dahnert2011, Hansell2008}.
%It is associated to the pleural space, which is the space between the pulmonary pleura and the costal pleura and is also called the pleural cavity \citep{Charalampidis2015}.
%Pneumothorax is the medical term for a pleural space that is filled with gas (often air) \citep{Dahnert2011,Hansell2008}.
%\loadFigure{Figure:pathologies}
If the mentioned pleural space is filled with an aggregation of fluid, the condition is referred to as a \textit{Pleural Effusion} \citep{Karkhanis2012}.
\textit{Lung Opacity} is a collective term and refers to lung areas that appear more opaque than the surrounding area \citep{Hansell2008}.
Because the lung appears dark on radiographs, opaque areas are white in contrast to the lung appearance.
It should be emphasized that the term is non-specific and is not descriptive in terms of neither the extent nor the origin of the pathology \citep{Hansell2008}.
\textit{Pneumonia} is another lung disease and refers as a broad term to an infection within the lung airspace or the interstitial lung tissue that also appears as a \textit{Lung Opacity} on chest X-rays \citep{Hansell2008}.
A \textit{Rib Fracture} is the term for a broken rib bone.
Sample images of the conditions are depicted in Figure \ref{fig:pathologies}.
%\loadFigure{Figure:pathologies} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Adapting SimCLR and DINO for Chest X-ray Images}
%\subsection{Methods and Evaluation Setup
\label{section: adapted-methods}
\subsubsection{Pretraining and Unsupervised OOD detection Framework}
SSL models are pretrained on settings 1-3 (see \ref{section: dataset}) and for a comparison on the entire CheXpert dataset ($\text{CheXpert}_\text{all}$).
During  DINO training, the mean accuracy of a 1-knn classifier on top of the ViT features is evaluated, to monitor the training progress \citep{Wu2018,Caron2021}.
Then, the projection head is removed \citep{Caron2021,Chen2020} and the features of the pretrained ViT encoder are used for unsupervised OOD detection.
The training and test sets are passed through the ViT encoder, and the $k$-NN feature similarity between training and test features is used as the unsupervised OOD detection score (see Equation \ref{equation:knn-score}).
\par
\subsubsection{Fine-tuning and Supervised OOD detection Framework}
To also compare with the supervised baseline of \citep{Berger2021}, the pretrained ViT weights are fine-tuned with a supervised MLP head on setting 1-3.
The MLP head consists of two fully connected layers with a hidden dimension of 256 and an output dimension set to the number of ID classes.
Between the linear layers, dropout is applied with a dropout rate of 0.5 \citep{Srivastava2014} followed by ReLU activation \citep{Agarap2018}.
The loss function is set to the cross-entropy loss. 
To detect OOD samples, MSP and Mahalanobis distance are used as supervised confidence scores (see section \ref{section: Out-of-Distribution Detection}).
Fine-tuned models are referred to as Sup ViT.
\par
\subsubsection{Augmentations for Pretraining}
\label{section: augmentations for pretraining}
Minor adjustments are made to the augmentations used for pretraining.
For example, \textit{Gaussian blur} is not applied because local textures and important pathological areas may not be well-defined if applied \citep{Azizi2021}.
Furthermore, saturation and hue are also set to zero for the \textit{random color distortions}, since hue and saturation are not meaningful for grayscale images with only one color channel.
The augmentations used for SimCLR are \textit{random resized crop} and \textit{random color distortions}.
If not mentioned otherwise, global crops scale and local crops scale of DINO models are copied from the pretraining arguments of the official \href{https://github.com/facebookresearch/dino#pretrained-models}{github} implementation and are set to [0.25, 1] and [0.05, 0.25] respectively. 
The augmentations used for DINO are \textit{random resized cropping}, \textit{random color distortions} and \textit{random horizontal flipping}.
In addition, an ablation study is evaluated and the (adapted) DINO augmentations are extended by a random rotation of $\pm$ 20 degrees and referred to as \textit{rotate augmentations}. 
This modification is inspired by \citep{Azizi2021} and is compared to the previously adapted augmentations.
%To fine-tune the Sup ViT, \textit{random resized crop}, \textit{random horizontal flip}, \textit{random color distortions} and \textit{random rotation} are used as augmentations and the crop scale is set to [0.8, 1].
\par
\subsubsection{Custom Augmentations for Setting 1}
Further, custom augmentations are designed to simulate pathologies on chest X-rays through image distortions.
They are designed for the \textit{Fracture} pathology and are used for setting 1.
The custom augmentations are applied to frontal images within bounding boxes shown in Figure \ref{fig:custom-augs-applied} and assume that the thorax is centered.
The bounding boxes should heuristically cover large portions of the lungs and rib cage.
The augmentations are applied with a probability of 0.5 to either the image's left side or right side.
In Figure \ref{fig:custom-augs-applied}, the green bounding boxes correspond to the borders of possible image slices where the image corruption is applied.
The red bounding boxes correspond to examples of possible image slices.
The custom augmentations are designed for an image size of $224\times224$ pixels, but with adjustments, any square size should be applicable.
\par
First, a probability p is sampled, which indicates on which side of the image the augmentation is applied.
Then, a horizontal starting pixel of the red slice is drawn from pixel values ranging from 50 to 100 on the left side and 134 to 184 on the right side and a vertical starting pixel of the red slice is sampled from a pixel range of 45 to 80 (about 20\% - 36\% of the total height).
Subsequently, a vertical ending point of the red slice is randomly chosen from pixel values ranging from 125 to 160 (about 56\% - 72\% of the total height).
Then, the horizontal width of the red bounding box is chosen as a uniform value between 2 pixels and 6 pixels.
Finally, a vertical offset value between 1 and 3 pixels is sampled, shifting the entire vertical slice represented by the red bounding boxes.
%\loadFigure{Figure:custom-augs-bbox}
The effect of the described image distortions is visualized in Figure \ref{fig:custom-augs-applied}.
The width of the simulated upper rib fracture (left side of the patient, right side of the image) is 6 pixel values, while the vertical offset is 2 pixel values.
Although the resolution is relatively low, the simulated \textit{Fracture} can be seen in the image by visual inspection and is marked by the blue arrow.
\loadFigure{Figure:custom-augs-applied}
\subsubsection{Implementation Details}
SimCLR models are trained for 150 epochs with a batch size of 1024.
The learning rate is first warmed up to 3.0e-04 for 10 epochs with a linear schedule \citep{Goyal2017} followed by a cosine decay schedule \citep{Chen2020,Loshchilov2016}. 
Gradients are accumulated over 5 steps because SimCLR profits from larger batch sizes \citep{Chen2020}.
The temperature of the loss function is set to 0.5. 
The training loss is optimized with Adam \citep{Kingma2014} and the weight decay is set to 1.0e-06.
Inputs are processed with a pixel size of $96\times96$.
\par
DINO models are trained for 100 epochs with a batch size of 128.
The learning rate is warmed up to its base value of $0.00075$ using the linear scaling rule \citep{Goyal2017}: $lr= 0.0015 * \text{batchsize}/256$ for 10 epochs \citep{Caron2021}.
Then, it follows a cosine decay schedule to 1.0e-05 \citep{Chen2020,Loshchilov2016}.
For the teacher network, a momentum of 0.996 is used and the teacher temperature $\tau_t$ is set to 0.04 using a linearly increasing schedule from 0.02 over 30 epochs.
The number of local crops is set to 10 and the number of global crops equals 2.
The output dimension is set to $D=12000$ and the loss is optimized using the AdamW optimizer \citep{Loshchilov2018} and a weight decay that follows a cosine schedule with a maximum of 0.4 and a base value of 0.04.
When training on $\text{CheXpert}_\text{all}$, $\tau_t$ is set to 0.07 and the maximum weight decay is replaced with 0.5.
Global crops are processed with an image size of $224\times224$ pixels and local crops are processed with an image size of $96\times96$.
\par
Sup ViTs are trained for 100 epochs with a batch size of 256 and a learning rate of 3.0e-04. 
Stochastic Gradient descent is used with a momentum of 0.9 and a weight decay of 0.0001 to minimize the loss.
The image size is set to $224\times224$ pixels.
\par
For outlier exposure the same setup is used as for the Sup ViT.
OOD samples are provided during training with a percentage of $\text{OOD}_\text{fraction}$ and are oversampled by a factor of $1/(\text{OOD}_\text{fraction})$.
The oversampling is adapted from \citep{Fort2021}, but the authors also correct for the different number of classes in the ID and OOD datasets.
This did not lead to satisfactory results and is therefore omitted.
\subsection{Experiments and Evaluation Metrics}
\label{section: experiments}
\textbf{Experiment 1: Image normalizations.} In the first experiment, two image normalizations are compared and applied to SimCLR and DINO.
Typically, the input images are normalized with the mean and standard deviation of the dataset.
In computer vision, the ImageNet dataset statistics \citep{Deng2009} are also often used for normalization.
However, unlike the ImageNet dataset, the CheXpert dataset consists of grayscale images with low input variability.
This raises the question of which normalization is suitable for the task of OOD detection if SSL is applied.
SimCLR and DINO are pretrained with ImageNet normalization and CheXpert normalization (dataset statistics of CheXpert) on the first setting.
\par
\textbf{Experiment 2: Choice of $k$ for feature similarity.} The performance of feature similarity as an unsupervised OOD score depends on the choice of the number of nearest neighbors $k$.
In the second experiment, the performance of feature similarity as an unsupervised OOD detection score is evaluated for different values of $k$.
Pretraining is performed on setting 1-3 and the pretrained models are evaluated on all settings.
\par
\textbf{Experiment 3: Adding rotations to the augmentations.}
The adopted DINO augmentations (see \ref{section: adapted-methods}) are used in the former experiments.
In the third experiment, a rotation of $\pm$ 20 degrees is added to the adopted DINO augmentations and the performance of both augmentation schemes is compared.
\par
\textbf{Experiment 4: Fine-tuning pretrained models.}
The fourth experiment evaluates how well the pretrained models perform OOD detection when fine-tuned.
Models were pretrained with DINO on the entire CheXpert dataset and on settings 2 and 3.
The pretrained model weights are used as initializations for fine-tuning a cross-entropy classifier on top of the ViT encoder (see \ref{section: adapted-methods}).
\par
\textbf{Experiment 5: Model size of ViT.}
In the fifth experiment, the ViT-Base variant (see Table \ref{table:vit-model-variants}) is used as a backbone and compared to the ViT-Small variant.
Both Sup ViT-S and Sup ViT-B are initialized with ImageNet DINO weights and are fine-tuned on the in-distributions of settings 2 and setting 3.
\par
\textbf{Experiment 6: Custom augmentations.}
Because setting 1 only contains healthy patient images as ID class, a supervised classifier is not directly applicable to this setting.
Instead, the supervised classifier is trained on top of the ViT encoder using custom image augmentations that should sensitize the classifier to the appearance of \textit{Fractures} in the images.
The custom augmentations are described in detail in section \ref{section: adapted-methods}.
In particular, the healthy patients are assigned a class label of 0 and the images with custom augmentations applied are assigned a class label of 1.
\par
\textbf{Experiment 7: Outlier exposure.}
Another experiment is designed for setting 1 to evaluate the performance of outlier exposure with different fractions of the OOD dataset.
The OOD dataset was limited to a maximum size of the ID dataset (10856 samples).
Different fractions of the OOD dataset are revealed to the classifier and the performance is evaluated on setting 1.
The fractions are defined as 1\%, 5\%, 10\%, 25\%, 50\% and 100\% of the OOD dataset and pretrained ViT DINO (CheXpert) weights are used as initialization.
Both unsupervised and supervised outlier exposure are applied (see \ref{section: Out-of-Distribution Detection}).
\par
\textbf{Evaluation Metrics.}
%\subsection{Evaluation Metrics}
All OOD scores are evaluated using the following metrics: area under the receiver operating characteristic curve (AUROC) and area under the precision-recall curve (AUPRC).
Training and test accuracy is reported whenever applicable.