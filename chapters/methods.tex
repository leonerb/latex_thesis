\section{Methods}
Vision Transformers (ViTs) form the backbone of all models that will be trained in this thesis.
They were introduced by \citep{Dosovitskiy2020} and transfer the concept of text transformers developed by \citep{Vaswani2017} to the image (classification) domain.
ViTs will be used in this thesis in conjunction with the DINO model objective, which resembles a paradigm of training SSL models on image data \citep{Caron2021}.
The name DINO stands for \textbf{di}stilled knowledge with \textbf{no} labels and consists of a student and a teacher network, where the teacher network is updated by the exponential moving average of the student's weights \citep{Caron2021}.
\subsection{Dataset}
In this thesis, the CheXpert dataset is used as a collection of chest X-ray images developed by \citep{Irvin2019}.
The following subsection will describe the dataset and summarize the label extraction methods of radiology reports of the authors in more detail.
Further, the reasoning behind the choice of used dataset splits is explained. 
CheXpert is an extensive collection of both frontal and lateral chest X-ray images with 224,316 images in total which were obtained from 65,240 patients.
\par
The dataset contains labels of 12 common chest- and lung pathologies, the control class "No Finding" and the "Support Devices" class which adds up to 14 total classes \citep{Irvin2019}.
The 14 classes of the dataset were selected due to both common occurrence in the radiologic reports and clinical importance \citep{Irvin2019} and refer to the definitions in the glossary of terms for thoracic imaging \citep{Hansell2008}.   
CheXpert is a multilabel dataset, which means that in each image multiple pathologies may be present.
For each image, the pathologies are labelled as positive (1), negative (0) or uncertain (-1), where uncertainty labels express both contradictions in the report and also diagnostic uncertainty of the radiologist who has written the report \citep{Irvin2019}.
\par
Labels were extracted from free text radiology reports in three stages (see figure \ref{fig:multilabel-struc}).
In the first step, observations were extracted from the reports with the help of a hand-selected list of common phrases, reviewed by different board-certified radiologists \citep{Irvin2019}.
The extracted observations are then passed to a second stage.
This stage is split into a three steps pipeline, where observations are matched against pre-negation uncertainty rules first, then against negation rules and finally against post-negation uncertainty rules \citep{Irvin2019}.
Only if an observation does not match any of the rules, it is assigned a positive label \citep{Irvin2019}.
Those rules are extracted through NLP techniques and details can be found in \citep{Irvin2019}.
In the last step, mentions are aggregated into a final label for each image and each class.
Observations with at least one positively labelled mention are assigned an aggregated positive label and an uncertainty label is selected if there is no positive mention and a minimum of one mention of uncertainty in the observations \citep{Irvin2019}.
Negative labels are only assigned if at least one mention label is negative \citep{Irvin2019}. 
If there is overall no mention of an observation, a \textit{blank} label is set as the label of the respective class \citep{Irvin2019}.
\loadFigure{Figure:multilabel-struc}
\par
Both the multilabel structure and the number of different classes of the CheXpert dataset leave room for designing different ID and OOD split settings.
For novel disease detection, \citep{Berger2021} developed two different settings for the CheXpert dataset and for the NIH ChestX-ray8 dataset \citep{Wang2017} another setting can be found in \citep{Cao2020}.
\par
In this thesis different settings are evaluated.
In the first setting, the No Finding class is used as the in-distribution class, while a total of six pathologies is used as the out-of-distribution class.
Specifically, the model is trained on the No Finding class and performance metrics are separately evaluated on each of the six classes.
The out-of-distribution classes are Cardiomegaly, Fracture, Lung Opacity, Pleural Effusion, Pneumothorax and Support Devices.
Because CheXpert labels are hierarchically structured \cite{Irvin2019}, the pathologies Atelectasis, Edema, Consolidation, Lung Lesion and Pneumonia can be considered as subcases of Lung Opacity \textcolor{red}{\citep{Hansell2008}}.
Therefore, in the first setting, that contrasts the No Finding class with the mentioned out-of-distribution classes, images of Atelectasis, Edema, Consolidation, Lung Lesion and Pneumonia were also added to the Lung Opacity class.
Subsequently, the model performance evaluated on Lung Opacity is an average over five subclasses and one superclass.
Further, the two different settings of \citep{Berger2021} are used.
The first one defines Cardiomegaly and Pneumothorax as in-distribution pathologies and Fracture as out-of-distribution class.
The other setting consists of the two in-distribution classes Lung Opacity and Pleural Effusion, which are evaluated against two out-of-distribution pathologies Fracture and Pneumonia.
Settings and dataset sizes are summarized in table \ref{table:settings-dataset-sizes}.
\loadTable{Table:settings-dataset-sizes}
\par
Medical imaging datasets differ from commonly used computer vision benchmarks like CIFAR-10 \citep{Krizhevsky2009a}, CIFAR-100 \citep{Krizhevsky2009b} or ImageNet \citep{Deng2009} in terms of appearance and variability of input images.
Chest-X-ray images for example are greyscale by default and as only the upper body part is included there is only low input variability.
Further, classifying the underlying pathologies is even challenging for certified board radiologists and also dependent on the class.
On the CheXpert dataset, the miss rate (False Negative Rate) for classifying Consolidation can range from 55\% to 52\% or 37\% depending on the human radiologist assessing the Xray-image \citep{Irvin2019}, where lower values indicate better performance.
On the Atelectasis class, expert performance might be as high as 92\% precision (Positive Predictive Value) with higher values corresponding to better performance [ibid.].
In addition to that, a study has shown that performance on the same pathology can also vary in-between X-ray datasets \textcolor{red}{\citep{Majkowska2020}}.
\par
Due to the differences between CheXpert and the frequently used benchmarks, medical domain knowledge is advantageous and can result in better performance when designing the model architecture or the data augmentations for specialized medical imaging datasets.
In the following paragraph, six different conditions are briefly discussed providing basic medical knowledge about the CheXpert labels contained in different ID-OOD settings in this master thesis.
The label names are taken from the CheXpert dataset \citep{Irvin2019} and adhere to the glossary \citep{Hansell2008} if mentioned there.
Sample images of the conditions are depicted in figure \ref{fig:pathologies}.
\loadFigure{Figure:pathologies} 
\par
Cardiomegaly is the first condition of interest.
It is a general term for heart enlargements, where the cardiothoracic ratio is often used as a measurement of the severity of size increase.
In \citep{Dahnert2011} the ratio is defined as the ratio between the heart size and the thorax width. 
Values between 0.45 - 0.55 are marked as mild cardiomegaly and values above 0.55 are considered to be severe cases of cardiomegaly [ibid.].
\par 
Pneumothorax is a medical condition related to the lung (pleura) and will be used as an in-distribution pathology.
It is associated to the pleural space, which is the space between the pulmonary pleura and the costal pleura and is also called the pleural cavity \citep{Charalampidis2015}.
Pneumothorax is the medical term for a pleural space that is filled with gas (often air) \citep{Dahnert2011,Hansell2008}.
If the mentioned pleural space is filled with an aggregation of fluid, the condition is referred to as a pleural effusion \citep{Karkhanis2012}.
Lung opacity is a collective term and refers to lung areas which appears more opaque than the surrounding area \citep{Hansell2008}.
Because the lung appears dark on x-ray images, opaque areas are white in contrast to the lung appearance.
It should be emphasized that the term is unspecific and is not descriptive in terms of neither the extent nor the origin of the pathology \citep{Hansell2008}.
Pneumonia is another lung disease and refers as a broad term to an infection within the lung airspace or the interstitial lung tissue, which also appears as a lung opacity on chest x-rays [ibid.].
A rib fracture is the term for a broken rib bone.
\subsection{Out-Of-Distribution Detection}
xx
\subsection{Vision Transformers}
Vision Transformers (ViTs) are the reformulation of the Transformer architecture \cite{Vaswani2017} to images.
Originally, sequences of natural language are split into tokens and then processed by the text Transformer [ibid.].  
Similarly, ViTs process images as a fixed number of flattened image patches \citep{Dosovitskiy2020}.
Although the Transformer is designed as an encoder-decoder architecture, the Transformer architecture is introduced with an emphasis on the encoder part in this thesis in the first part of the section as only the encoder is used within the ViT architecture.
Then, minor adjustments that are needed to accept images rather than text as inputs are discussed.
The following part is fully based on \cite{Vaswani2017,Dosovitskiy2020}.
\par
The input sequence is assumed to be a sequence $(x_1,\dots,x_n)$, which is mapped to a continuous representation $z=(z_1,\dots,z_n)$ by the Encoder.
The Encoder is an Encoder \textbf{stack} and consists of a stack of $L$ layers.
The outputs of each layer in the encoder (including the embedding layers) are of size $d_{model}$ \citep{Vaswani2017}.
At the bottom of the Encoder, the input is embedded into a $d_{model}$ dimensional embedding space and then positional encodings are added.
As neither recurrent connections nor convolution is part of the Transformer architecture, positional encodings are necessary to constrain the model into learning about relative and absolute locational information within the sequence \citep{Vaswani2017}.
The authors experimented with learning positional encodings \citep{Gehring2018}, but decided to stay with sinusoidal positional encodings, as it theoretically allows the model to generalize to larger, unseen sequence lengths not seen in training \citep{Vaswani2017}.
The positional encodings depend on both the position $pos$ of the element within the sequence \textbf{and} on the parity of the dimension $i$ of the i-th component of the embedding vector.
So, depending on if $i$ is an odd or even number, the following functions are applied to retrieve a positional encoding \citep{Vaswani2017}:
\begin{equation}
	PE(pos, i) = 
	\begin{cases}
		sin(\frac{pos}{10000^{2i / d_{model}}}), i \text{ is even} \\
		cos(\frac{pos}{10000^{2i / d_{model}}}), \text{ else}
	\end{cases}
\end{equation}
\par
The input (embedding + positional encoding) is further processed by two sublayers and each sublayer is wrapped with a residual connection \citep{He2016}, which is subsequently transformed by a layer normalization \citep{Ba2016}.
This means that the normalization is applied on x + Sublayer(x), where Sublayer(x) is the output of the respective sublayer \citep{Vaswani2017}.
Specifically, applying a normalization layer to a vector $u\in \mathbb{R}^d$ corresponds to the following computation \citep{Ba2016,Xiong2020}:
\begin{align}
	\text{LayerNorm}(u) = \gamma \frac{u-\mu}{\sigma}+\beta,\\
	\mu = \frac{1}{d}\sum_{k=1}^{d}u_k, \sigma^2 = \frac{1}{d}\sum_{k=1}^{d}(u_k - \mu)^2
\end{align}
The mean and standard deviation is denoted as $\mu$ and $\sigma$, $\gamma$ is a scale parameter and $\beta$ is a bias vector \citep{Xiong2020}.
\par
The first sublayer of the encoder architecture is a multi-head attention layer.   
Multi-head attention layers are one of the main components of the Transformer architecture and prerequire the concept of attention \citep{Vaswani2017}.
Attention mechanisms are functions, which map three input vectors $q,k,v$ called query, keys and values to an output \citep{Vaswani2017}.
This output is a linear combination of the values $v$ where the specific weights for each component $v_i$ are obtained by a similarity function between the query and the corresponding keys \citep{Vaswani2017}.
The specific attention function which is used is named "Scaled Dot-Product Attention" and implemented by the following formula \citep{Vaswani2017}:
\begin{align}
    \text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V
	\label{equation:attention}
\end{align}
The three matrices $Q,K,V$ consist of the stack of vectors which are processed together and are called queries, keys and values, which is similar to before.
Each vector component of $Q$ and $K$ is $d_k$ dimensional and the matrix $V$ is a stack of $d_v$ dimensional vectors.
Equation \ref{equation:attention} is a specific adaption of dot-product attention with a scaling factor that is the reciprocal of the square-root of the dimension $d_k$, which helps to mitigate the risk of vanishing softmax gradients for larger values of $d_k$, according to \citep{Vaswani2017}.
The dot-product is realized through the matrix multiplication $QK^T$.
A multi-head attention layer now builds upon the mentioned attention mechanism by concatenating the output of $h$ different attention functions.
Queries, keys and values are mapped onto $h$ distinct subspaces with weight matrices $W_i^Q$, $W_i^K$, $W_i^V$ where $i=1,\dots,h$ \citep{Vaswani2017}.
This enables the model to mutually attend to the knowledge of various representation subspaces at different positions \citep{Vaswani2017}:
\begin{align}
	\text{MultiHead}(Q,K,V) &= \text{Concat}(\text{head}_1,\dots,\text{head}_h)W^O, \\
	\text{head}_i & = \text{Attention}(QW_i^Q,KW_i^K,VW_i^V) \\
	& W_{i}^{Q}, W_{i}^{k} \in \mathbb{R}^{d_{model}\times d_k} , W_i^V \in \mathbb{R}^{d_{model} \times d_v}	
\end{align}
After concatenating the attention of each head, the outputs are projected back onto the model dimension $d_{model}$ with the matrix $W^O \in \mathbb{R}^{hd_v \times d_{model}}$ and passed to the second sublayer of the Transformer encoder block.
\par
The second sublayer is a fully connected neural network with two linear layers, that are separated by a ReLU activation \citep{Vaswani2017} and the output of this feed forward network is also wrapped by a residual connection, followed by layer normalization as explained before.
Both in- and output dimension are set to $d_{model}$ and the inner dimension (output dimension of the first layer and in dimension of the second layer) is set to $d_{ff}$ [ibid.].
Thus, the second sublayer can be described by the following formula:
\begin{equation}
	\text{FFN}(x) = \max(0, xW_1 + b_1)W_2+b_2 
\end{equation}
\loadFigure{Figure:vit-arch}
\subsection{Model Architecture}
A DINO head will be used to obtain weights for the ViT backbone.
The notation and terminology in the following subsection are adapted from \citep{Caron2021}.
As discussed by the authors in \citep{Caron2021}, the DINO head is essentially a teacher-student network.
Both teacher network and student network are parametrized by a weight vector $\theta_t$ and $\theta_s$ respectively.
The output $g_{\theta_s}$ of the student network is trained to equal the output $g_{\theta_t}$ of the teacher network, which is described as "SSL with Knowledge Distillation" by the authors \citep{Caron2021}.
An input image $x$ is passed through each network, resulting in the output which is used to calculate probability distributions $P_s$ and $P_t$ over $K$ variables for both networks, where the probability distributions are calculated as the normalized output of $g_{\theta_s}$ and of $g_{\theta_t}$:

\begin{align}
	P_s(x)^i = \frac{\exp(g_{\theta_s}(x)^i/\tau_s)}{\sum_{k=1}^{k=K}\exp(g_{\theta_s}(x)^k/\tau_s)}, \hspace{8pt} i=1,\dots,K
	\label{methods-softmax}
\end{align}

The normalization that is used is the softmax function (see \ref{methods-softmax}) and the $\tau_s > 0$ is a temperature parameter which influences the shape of the output distribution by manipulating the confidence of the predictions [QUELLE einfügen und mehr erläutern].
Note, that the exact number of dimensions $K$ is unknown, as no label knowledge is directly incorporated.
In the experiments $K$ will be set to \dots
\\
Let $H(p_1,p_2)$ denote the cross entropy of two probability distributions $p_1$, $p_2$, i.e. $H(p_1,p_2)=-p_1 \log p_2$.
Further, assume a fixed teacher network parametrized by $\theta_t$.
Matching the output of the student network to the output of the teacher network can be reformulated as minimizing the cross-entropy of the teacher- and student distributions w.r.t. $P_s$:

\begin{align}
	\hat{\theta}_s = \min_{\theta_s}H(P_t(x),P_s(x))
\end{align}

The authors integrate SSL by adopting a multi-crop strategy [QUELLE].
Given the original input image $x$, several views of the same image are generated by applying augmentations to $x$.
The images of the set of augmentations called $V$ represent manipulated views of $x$, while global and local views correspond to views that enclose larger parts (e.g. >50\% receptive field(?)) and smaller parts (e.g. <50\% receptive field(?)) respectively [QUELLE].

\begin{align}
	\min_{\theta_s}\sum_{x \in \{x_1^g, x_2^g\}} \hspace{5pt} \sum_{\substack{ x'\in V, \\ x\neq x'}} H(P_t(x),P_s(x'))
	\label{methods-training-obj}
\end{align}

Equation \ref{methods-training-obj} is the training objective of the DINO head, which also reveals the setup of the feed-forward pass:
The outer sum collects all global views $x$, while the inner sum goes over all local views $x'\neq x$.
The global views are only passed to the teacher network and the local views are passed to the student network.
From these global-to-local correspondence [QUELLE, mehr zu cross entropy ergänzen] the teacher and student distribution should be as close as possible (entropy-wise).
\\
In this thesis, the number of global views is restricted to two and several local views are assumed, which adheres to the logic of the authors in \citep{Caron2021}. 
The parameter weights of the student network $\theta_s$ are updated by stochastic gradient descent with learning rate $\eta$ and fixed teacher network, i.e. [QUELLE]:

\begin{align}
	\theta_s \leftarrow \theta_s - \eta \nabla H(\bar{P}_t(x),P_s(x))
\end{align}

The parameter weights of $g_{\theta_t}$ are obtained by calculating an exponential moving average \citep{Grill2020,Caron2021}: 

\begin{align}
	\theta_t(T+1) \leftarrow \lambda \theta_t(T) + (1-\lambda) \theta_s(T).
	\label{methods-momentum-encoder}
\end{align}

A new estimate of $\theta_t$ at iteration $T+1$ is the result of the $\lambda$ weighted convex combination of the current value of $\theta_t$ and $\theta_s$ at time $T$ respectively.
The update rule in \ref{methods-momentum-encoder} is the same as a momentum encoder \citep{He2019} and $\lambda$ is set according to a cosine schedule \citep{Grill2020}.
\loadFigure{Figure:simclr-arch}
% \loadTable{Table:dino-head-dim}
% \loadTable{Table:normalizations}
% \loadTable{Table:weights-init-method}
% \loadFigure{Figure:labels-used-AUC-setting1}
% \loadFigure{Figure:labels-used-AUC-setting2}
% \loadFigure{Figure:heatmap-geom}
% \loadFigure{Figure:heatmap-spatial}