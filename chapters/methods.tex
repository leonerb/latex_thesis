\section{Methods}
Vision Transformers (ViTs) form the backbone of all models that will be trained in this thesis.
They were introduced by \citep{Dosovitskiy2020} and transfer the concept of text transformers developed by \citep{Vaswani2017} to the image (classification) domain.
ViTs will be used in this thesis in conjunction with the DINO model objective, which resembles a paradigm of training SSL models on image data \citep{Caron2021}.
The name DINO stands for \textbf{di}stilled knowledge with \textbf{no} labels and consists of a student and a teacher network, where the teacher network is updated by the exponenial moving average of the student's weights \citep{Caron2021}.

\subsection{Vision Transformers}
Vision Transformers are based on the transformer architecture and reformulate the attention mechanism designed in \cite{Vaswani2017} to images.
Whereas text transformer split a sequence of natural language into tokens, Vision Transformers split images into a fixed number of image patches \citep{Dosovitskiy2020}.
\\
The flattened image patches represent the token sequence which is passed to the transformer encoder architecture.
The encoder itself consists of a number of stacked transformer blocks, where each block contains linear layers, normalization layers and attention \citep{Vaswani2017,Dosovitskiy2020}.
\\
These patches are fed into a neural network architecture that consists of a number of stacked transformer blocks, where each block contains linear layers, normalization layers and attention \textcolor{red}{?}
By design, the original patch order does not matter, which means that positional encodings need to be added to establish an order \citep{Dosovitskiy2020,Vaswani2017}.
\begin{figure}[ht]
	\centering
    \includegraphics[width=\textwidth]{diagrams/vit_arch.drawio.pdf}
	\caption{Architecture of ViT Transformer. Adapted from \citep{Dosovitskiy2020}}
	\label{intro-vit-arch}
\end{figure}
\\
Attention is implemented by the following formula \citep{Vaswani2017}:
\begin{align}
    \text{attention}(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V
\end{align}
The three matrices $Q,K,V$ are called query, keys and values respectively.
To understand what it is really doing it helps to look at the two components.
In a first step, the inner product between all queries $Q$ and all keys $K$ is considered.
This is rescaled with $\sqrt{d_k}$ and applying the softmax function consecutively results in a probability distribution over the elements.
The resulting matrix acts as an attending algorithm and can be thought of as a covariance matrix, where higher values in $(i,j)$ should indicate that the architecture should attend to the $j$-th token, when processing token $i$ \citep{Vaswani2017} \textcolor{red}{???}.    
\\
\\
\subsection{Model Architecture}
A DINO head will be used to obtain weights for the ViT backbone.
The notation and terminology in the following subsection are adapted from \citep{Caron2021}.
As discussed by the authors in \citep{Caron2021}, the DINO head is essentially a teacher-student network.
Both teacher network and student network are parametrized by a weight vector $\theta_t$ and $\theta_s$ respectively.
The output $g_{\theta_s}$ of the student network is trained to equal the output $g_{\theta_t}$ of the teacher network, which is described as "SSL with Knowledge Distillation" by the authors \citep{Caron2021}.
An input image $x$ is passed through each network, resulting in the output which is used to calculate probability distributions $P_s$ and $P_t$ over $K$ variables for both networks, where the probability distributions are calculated as the normalized output of $g_{\theta_s}$ and of $g_{\theta_t}$:

\begin{align}
	P_s(x)^i = \frac{\exp(g_{\theta_s}(x)^i/\tau_s)}{\sum_{k=1}^{k=K}\exp(g_{\theta_s}(x)^k/\tau_s)}, \hspace{8pt} i=1,\dots,K
	\label{intro-softmax}
\end{align}

The normalization that is used is the softmax function (see \ref{intro-softmax}) and the $\tau_s > 0$ is a temperature parameter which influences the shape of the output distribution by manipulating the confidence of the predictions [QUELLE einfügen und mehr erläutern].
Note, that the exact number of dimensions $K$ is unknown, as no label knowledge is directly incorporated.
In the experiments $K$ will be set to \dots
\\
Let $H(p_1,p_2)$ denote the cross entropy of two probability distributions $p_1$, $p_2$, i.e. $H(p_1,p_2)=-p_1 \log p_2$.
Further assume a fixed teacher network parametrized by $\theta_t$.
Matching the output of the student network to the output of the teacher network can be reformulated as minimizing the cross-entropy of the teacher- and student distributions w.r.t. $P_s$:

\begin{align}
	\hat{\theta}_s \leftarrow \min_{\theta_s}H(P_t(x),P_s(x))
\end{align}