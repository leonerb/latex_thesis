\section{Results}
\label{section: results}
In the last section all experiments are described in detail.
This section is dedicated to the results of the experiments.
In the first part, models are pretrained on setting 1-3 and on the whole CheXpert dataset.
After evaluating the unsupervised OOD capabilities, the pretrained models are fine-tuned and supervised results are evaluated to compare with \citep{Berger2021}.
Last, the results of outlier exposure are shown.
\par
First, different training architectures and model configurations are evaluated on the setting 1 dataset split.
As this split contains six different OOD pathologies, including the ones from the other settings (see table \ref{table:settings-dataset-sizes}), the best configuration found for setting 1 was then applied to the other two settings.
\par
In the first experiment, two different image normalizations are compared and applied to SimCLR and DINO.
Figure \ref{fig:setting1-chexnorm-v-imgnorm} summarizes the results.
In table \ref{fig:setting1-chexnorm-v-imgnorm-dino-last-epoch}, SimCLR and DINO models are compared for both normalizations.
If restricted to SimCLR and averaged across the OOD pathologies, the AUROC based on the feature similarity of the ViT encoder is 0.26 percentage points higher for input normalized with CheXpert statistics.
If pretrained with DINO, the AUROC is 0.66 percentage points higher for ImageNet normalized input.
As indicated by the bold values, the ViT features of the discarded DINO projection head outperform the features of the SimCLR architecture for each of the six OOD pathologies.
Figure \ref{fig:setting1-chexnorm-v-imgnorm-dino-barplot} shows this for the ImageNet normalization.
The AUROC values are consistently above 50\% for all OOD pathologies with a maximum of 61.23\% for the OOD pathology Pneumothorax.
%\par
% Apart from the performance, the low standard deviation of the CheXpert dataset statistics ($\sigma=0.0349$), leads to a normalized input range of approximately [-17,14].
% Therefore, to mitigate the risk of convergence problems for high input values \citep{Lecun2002, He2015, Santurkar2019} and because of higher performance, the ImageNet normalization is used for all following experiments.
Further experiments with SimCLR are omitted due to the outperformance of the DINO model.
\loadFigure{Figure:setting1-chexnorm-v-imgnorm}
% \par
% The performance of feature similarity as OOD score is dependent on the choice of the number of nearest neighbours $k$.
% It is analysed how the performance changes with different values for $\tau$ and $k$.
% As a DINO model was already pretrained on setting 1, the pretrained model weights are used as initializations to evaluate the mentioned setting.
% Further, additional DINO models are pretrained on setting 2 and setting 3.
\par
In experiment two, the OOD score consistency of the feature similarity was analysed for all three settings.
The results are summarized in figure \ref{fig:auroc-vs-k} and reveal the following findings:
While the performance is slightly worse for the first two settings with an increased value of $k$, the performance on setting 3 increases, when more nearest-neighbours are used for the feature similarity.
No clear consistency across the settings is observable and the number of neighbours $k$ is set to 1, if setting 1 or 2 are evaluated. 
For setting 3, $k$ is set to $N_3$, which is the length of the respective ID training set (Lung Opacity and Pleural Effusion).
\loadFigure{Figure:auroc-vs-k}
\par
% The impact of different image augmentations is now discussed.
% In the former experiments, the adopted DINO augmentations (see \ref{section: adapted-methods}) were used.
% The authors in \citep{Azizi2021} added a rotation of 20 degrees to their augmentations.
% We will also add a rotation of 20 degrees to the adopted DINO augmentations and compare the performance of both augmentation schemes.
% \par
Figure \ref{fig:dino-rotate-augs-v-default-augs} summarizes the results of experiment 3. 
A rotation by 20 degrees was added to the adopted DINO augmentations and the performance is compared to the augmentations without rotation.
For unsupervised OOD detection, the DINO augmentations outperform the rotation augmentations across all settings.
However, the 1-NN mean accuracy is significantly higher on setting 2, if additional rotations are applied to the CheXpert images.
In setting 3 no significant difference is observable for the training accuracy.
Thus, a clear correlation between unsupervised OOD detection and 1-NN mean accuracy is not observable.
The performance of both augmentation schemes was tested for fine-tuning experiments and the additional rotation lead to a higher ID accuracy and a higher OoDD performance. 
In the remaining thesis, a rotation is added to the pretraining augmentations in all experiments.
\loadFigure{Figure:dino-rotate-augs-v-default-augs}
\par
It is often insightful to visualize the self-attention maps of the ViT encoder.
The self-attention of the \texttt{[CLS]} token of the last layer is used for visualizing the output of each head \citep{Caron2021,Dosovitskiy2020}.
For each of the in-distribution classes, an input image of the respective class is passed to the ViT encoder that was pretrained on setting 1-3.
Pixel regions are marked with respect to self-attention values.
Results for all six attention heads are summarized in \ref{table:attention-maps}.
It should be emphasized, that the rows correspond to different models.
For example, the second and third row show the ViT attention maps for the Cardiomegaly and Pneumothorax class pretrained with DINO on setting 2 with the respective ID classes.
The same holds for the first row (pretrained on setting 1) and for the fourth and fifth row (pretrained on setting 3).
In our case, blue colormap values correspond to regions of the lowest attention, while yellow values indicate the regions of interest of the ViT encoder with higher attention values.
\par
The results show, that while some basic properties of chest X-rays are expressed through higher attention values, most heads do not attend to the region of interest.
The ViT encoder attends for example only to random pixel regions for the No Finding class.
In contrast to that, $\text{head}_5$ seems to attach to some lung parts for the Lung Opacity pathology.
There is no evidence, that pretraining on setting 1-3 lead to segmentation capabilities for the ViT encoder.
Limiting the pretraining dataset to the narrow ID classes of the settings seems to inhibit the ViT encoder from learning meaningful representations.
\loadTable{Table:attention-maps}
\par
In the former part, the unsupervised performance of the pretrained models was investigated.
It is now analysed in experiment 4 how well the pretrained models perform, if labels are available to fine-tune a cross-entropy based classifier on top of the ViT encoder.
%To that end, another DINO model was pretrained on the whole CheXpert dataset.
% The pretrained model weights are used as initializations for finetuning a cross-entropy based classifier on top of the ViT encoder.
In this case, the performance is only evaluated on settings 2-3 in the first part, because setting 1 contains only 1 ID class and therefore no direct supervised classifier can be trained without modifications.
Setting 1 will be revisited later and the performance of a supervised classifier on this setting is evaluated if custom augmentations are applied (thereby creating a second class next to the No Finding class) or if fractions of the OOD dataset are made available.
\par
%With label access to the ID training set, supervised OOD detection is evaluated with MSP and Mahahalanobis distance as OOD score (see \ref{section: Out-of-Distribution Detection}).
The results are summarized in table \ref{table:finetuning-supervised-new}.
For setting 2, pretraining on the whole CheXpert dataset outperforms the other models, reaching up to 70.8\% AUROC and 71.6\% AUPRC.
The supervised baseline in \citep{Berger2021} performs worse with 58 \% AUROC and 69.5 \% AUPRC respectively, but the authors report a higher test accuracy of 88.8 \% against 87.9 \% for the Sup ViT model.
Pretraining on setting 2 only leads 63.4 \% AUROC and 64.5 \% AUPRC if the Mahahalanobis distance is used as the OOD score.
This performance is comparable to fine-tuning model weights pretrained on the ImageNet dataset, suggesting that pretraining on the settings leads to no measurable performance increase against using ImageNet weights.
Fine-tuning the supervised classifier from random weights leads to the worst performance of 56.7 \% AUROC and 56.1 \% AUPRC.
If restricted to the best performing model, a clear difference between applying the Mahahalanobis distance in the feature space of the ViT encoder and in the penultimate layer of the classifier is not observable.
The Mahahalanobis distance is therefore only calculated in the feature space of the ViT encoder in the following experiments.
\par
Similar results hold for setting 3. However, the supervised baseline outperforms our model that was pretrained on the whole CheXpert dataset if the AUPRC is used as a performance metric (60.1 \% vs. 56.5\%).
Pretraining on the CheXpert dataset leads to higher AUROC values (54.2 \% vs. 45.8 \%), if the MSP is used as an OOD score. 
The performance on setting 3 is worse than on setting 2 and most AUROC- and AUPRC values are close to random guessing of 50\%.
%Finetuning on setting 2 increased the performance, while finetuning on setting 3 even decreased the performance.
% This is because the OOD class Pneumonia used in setting 3 is a subclass of the ID pathology Lung Opacity (see figure \ref{fig:multilabel-struc}) and although no OOD labels were exposed to the classifier during training, the OOD detection performance might decrease with increasing accuracy, as the model does not distinguish between both classes.
% Therefore, the following experiments will be carried out only on setting 1 and setting 2.
\loadTable{Table:finetuning-supervised-new}
\par
Until now, only the ViT-S/16 was used as a backbone.
In experiment 5, the ViT-S/16 and ViT-B/16 architectures are compared.
The results are summarized in table \ref{table:vit-model-variants}.
On setting 2, the MSP based AUROC is the same across both model sizes with 65.9 \% AUROC.
Based on the AUPRC, the Sup ViT-B model outperforms the Sup ViT-S model with 67.9 \% AUPRC against 66.8 \% AUPRC.
If the Mahahalanobis distance is used as OOD score, then the AUROC of the is 3 percentage points higher for the Sup ViT-B (66.7\% and 63.3\%).
\par
For setting 3, AUPRC and AUROC are slightly higher for the smaller architecture based on the MSP.
However, the Sup ViT-B performs better than the Sup ViT-S, if the Mahahalanobis distance is used as an OOD score.
Still the performance is worse than on setting 2 and close to random guessing.
Due to only minor performance gains and the increased model size, all following experiments are restricted to the ViT-S architecture.
\loadTable{Table:vit-imagenet-model-size}
%\loadTable{Table:supcon-results}
\par
\loadFigure{Figure:supervised-vs-unsupervised}
In the former part of the results section, the unsupervised OOD detection performance of settings 1-3 was evaluated.
Supervised results for setting 2 and setting 3 are compared to the supervised baseline and ODIN in figure \ref{fig:supervised-vs-unsupervised} and unsupervised results are depicted as a reference.
For setting 2, the proposed Sup ViT outperforms the supervised baseline by 3 percentage points in terms of AUROC.
Still, the performance of ODIN is 13 percentage points higher than the performance of the Sup ViT model.
As already mentioned in \ref{section: Out-of-Distribution Detection}, the performance of ODIN is comparable to the other supervised methods because no access to OOD labels was assumed in \citep{Berger2021}.
For setting 3, the Sup ViT model underperforms the supervised baseline by 4 percentage points in terms of AUROC.
In addition to that, the supervised performance is lower than the unsupervised performance, suggesting that fine-tuning on setting 3 degrades the OOD detection performance.
The AUROC of ODIN is also higher as before and beats the Sup ViT model by a wide margin.
\par
In experiment 6 custom augmentations are applied to the setting 1 dataset split.
Because the setting 1 only contains healthy patient images as in-distribution class, a supervised classifier is trained on top of the ViT encoder using custom image augmentations, that should sensitize the classifier to the appearance of fractures in the images.
The custom augmentations are described in detail in section \ref{section: adapted-methods}.
As depicted in table \ref{table:custom-augs-fracture-results} with different initializations, the performance is slightly worse than the prior performance of 55.69 \% AUROC  and 55.0 \% AUPRC (see \ref{fig:setting1-chexnorm-v-imgnorm-dino-last-epoch}).
If pretrained on CheXpert, the AUROC and AUPRC values are 52.2 \% and 52.9 \% respectively. 
Even pretraining on $\text{CheXpert}_\text{Setting 1}$ which in-distribution consists of healthy patients leads to a performance that is close to a random guess.
The same holds for random initializations.
\loadTable{Table:custom-augs-fracture-results}
\par
Although technically the described augmentations can also be generalized to other pathologies, further experiments are omitted.
No performance increase is observed and manually designing augmentations for each pathology is cumbersome.
\par
The focus in the last experiments is now shifted towards outlier exposure on setting 1 and a small subsample of OOD images is exposed during training (see section \ref{section: Out-of-Distribution Detection}).
% First, the OOD dataset was limited to a maximum size of the ID dataset (10856 samples).
% Different fractions of the OOD dataset are made available to the classifier and the performance is evaluated on setting 1.
% The fractions are chosen to be 1\%, 5\%, 10\%, 25\%, 50\% and 100\% of the OOD dataset and the pretrained weights of $\text{CheXpert}_\text{all}$ are used as initializations.
% Both unsupervised outlier exposure and supervised outlier exposure are applied (see \ref{section: Out-of-Distribution Detection}).
First, the performance across the OOD pathologies Pleural Effusion, Cardiomegaly and Pneumothorax is evaluated.
\loadFigure{Figure:ood-fraction-avg}
\par
Even for small fractions of OOD data (1\% and 5\%), the performance increases compared to the unsupervised baseline (ViT pretrained with DINO on setting 1).
For reference, 1\% and 5 \% of the OOD dataset correspond to 109 and 547 samples, respectively.
The performance increases with higher fractions of OOD data and reaches a maximum of 81.7\% AUROC (unsupervised outlier exposure) and 82.2\% AUROC (supervised outlier exposure), if the whole OOD dataset is available.
Supervised outlier exposure did not lead to a clear performance increase over unsupervised outlier exposure, which implies that fine-grained labels are not necessary for outlier exposure on chest-X-rays.
The results are summarized in figure \ref{fig:ood-fraction-avg}.
\loadFigure{Figure:ood-fraction-fracture}
\par
In the following, the performance on Fracture is discussed.
In contrast to the other three mentioned OOD classes Pleural Effusion, Cardiomegaly and Pneumothorax, outlier exposure on Fracture only leads to minor performance increases.
Even if the whole OOD dataset is available, the performance reaches a maximum of 59.0 \% AUROC (unsupervised outlier exposure) and 59.9 \% AUROC (supervised outlier exposure), which is only slightly above the baseline performance of 55.7 \% AUROC.
The results are summarized in figure \ref{fig:ood-fraction-fracture} and suggest that the classifier is not able to distinguish between the ID and OOD class and that performance of outlier exposure deviates across the OOD pathologies.  